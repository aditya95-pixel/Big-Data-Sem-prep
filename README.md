# Big Data Semester Preparation

## Group A

### Module 1

### (i)Big data is a term for a collection of data set, so large and complex that it becomes difficult to process using 	

- (a) Cloud
- ✅ **(b) Traditional data processing application**
- (c) Visualization
- (d) Unstructured Data 

### (ii)Challenges of Big data faced by IBM are 	
- (a) 3Es
- ✅ **(b) 3Vs**
- (c) 3Os
- (d) 3Ls
### (iii)Challenges of Big Data include 	
- (a) Captute, Curation 
- (b) Storage ,Search , Sharing 
- (c) Transfer ,analysis and Visualization 
- ✅ **(d) All the above** 

### (iv)Earthscope designed to track North America’s geologic evolution reports as 
- ✅ **(a) Repository** 
- (b) Source 
- (c) Process unit 
- (d) None of the Above 
### (v)Real Time fast data is generated from 	
- (a) Social media Networks 
- (b) Scientific Instruments and mobile Devices 
- (c) Sensor technology and Networks 
- ✅ **(d) All the above** 

### (vi)Example of Semi structured data is 	
- (a) Legacy Data 
- (b) Web 
- ✅ **(c) XML**
- (d) None of the Above 
### (vii)Valence in Big data corresponds to 	
- (a) Dimension by data 
- ✅ **(b) Connectedness**
- (c) Speed 
- (d) Complexity 
### (viii)	Data can be harnessed by 
- (a) OLTP and OLAP
- (b) OLAP and RTAP
- ✅ **(c) Both (a) and (b)** 
- (d) None of the Above 

### Module 2
### (i)Apache Hadoop 	
- ✅ **(a) Open source software framework for Big Data** 
- (b) Storage system of Hadoop for Big Data 
- (c) Programming Model for Big Data 
- (d) Map Reduce Summarizer for Big data 
### (ii)Giraph is 	
- (a) Specialized model for Reduce function 
- ✅ **(b) Specialized model for graph processing** 
- (c) Higher level interactivity using storm 
- (d) None of the above 
### (iii)HDFS is a 	
- (a) Application 
- (b) Map Reduce 
- ✅ **(c) Distributed file system that runs on clusters.**
- (d) Non map reduce application 
### (iv) Streaming Applications are done using 	
- (a) Star, Spark 
- (b) Spark, Flink and HDFS
- ✅ **(c) Both (a) and (b)**
- (d) None of the above 
### (v) Pig and Hive are tools 	
- (a) That simplify programming of Map reduce 
- (b) Deals with scripts and queries 
- ✅ **(c) Both (a) and (b)** 
- (d) None of the above 
### (vi) Major Phases of MapReduce are  	
- (a)Mapping ,Reducing,  
- (b) Shuffling ,Sorting, Combining 
- ✅ **(c)Mapping , Shuffling , Sorting , Reducing**  
- (d) Both (a) and (b)
### (vii)Map Reduce Engine 	
- (a) Has cluster and slaves 
- ✅ **(b) Consists Job Tracker and task Tracker** 
- (c) Keeps the balance 
- (d) 
### (viii) MapReduce application end users can run on top of 	
- (a) YARN without disrupting any processes
- ✅ **(b) YARN , NextGen YARN**
- (c) Client 
- (d) Server 

### Module 3
### (i) Characteristics of IoT are:
- (a) Dynamic and Self Adapting  
- (b) Interoperable communication protocols  
- ✅ **(c) Both (a) and (b)**  
- (d) None of the above  

### (ii) Each IoT device has a Unique Id:
- ✅ **(a) Unique Identifier**  
- (b) Internet Protocol Address  
- (c) Personal Computer  
- (d) Smart Phone  

### (iii) Applications of IoT are:
- (a) Music  
- (b) Home lighting  
- (c) Analytics  
- ✅ **(d) All the above**  

### (iv) IoT device can:
- ✅ **(a) Exchange data with other connected devices and applications**  
- (b) Cannot collect data from other devices  
- (c) Both (a) and (b)  
- (d) None of the above  

### (v) HDMI is:
- (a) High definition multiplier interface  
- ✅ **(b) High definition multimedia interface**  
- (c) High difference media interface  
- (d) None of the above  

### (vi) IEEE 802.11 is for:
- (a) Ethernet  
- (b) WPAN  
- ✅ **(c) WiFi**  
- (d) Mobile Communication  

### (vii) IPv4: Internet Protocol version 4 identifies:
- ✅ **(a) Identifies the devices in the network using hierarchical addressing scheme, total 64 addresses.**   
- (b) 128-bit address scheme  
- (c) Data transfer 250kb/s  
- (d) None of the above 

### (viii) CoAP, MQTT, XMPP, DDS are:
- ✅ **(a) Application Layer Protocol**  
- (b) Presentation Layer Protocol  
- (c) Session Layer Protocol  
- (d) Data Link Layer Protocol  

### Module 4
### (i)It uses HiveQl for data structuring and for writing complicated MapReduce in HDFS	
- (a) Storm
- (b) HDFS
- ✅ **(c) Hive** 
- (d) Pig 
### (ii)	Advantages of Hadoop are 
- (a) Ability to store large amount of data 
- (b)  high flexibility 
- ✅ **(c) Both (a) and (b)** 
- (d) None of the above 
### (iii)	HBase support 
- ✅ **(a) random read and writes**
- (b) write once read many times 
- (c) Both (a) and (b) 
- (d) All the above 
### (iv)	Zookeeper provides services as 
- (a) Maintaining and configuration of information 
- (b) Naming and provide distributed synchronization 
- ✅ **(c) Both (a) and (b)**
- (d) None of the above.
### (v)Web Socket API uses 	
- ✅ **(a) Client Server** 
- (b) Request response model 
- (c) Exclusive Pair model
- (d) None of the above 
### (vi) Paas is 	
- (a) Paradigm as a Service
- (b) Internet Provider as a Service 
- ✅ **(c) Platform as a Service**
- (d) None of the above 
### (vii)	Full duplex communication between client and server takes place in 
- (a) Stateless/ Stateful
- (b) Request Response 
- (c) TCP connections 
- ✅ **(d) Web Sockets**
### (viii)	Coordinator nodes collect data from end nodes and send to the 
- (a) Client
- ✅ **(b) Server** 
- (c) Cloud
- (d) None of the above 

---
### Fill in the blanks

1. Big data refers to a massive amount of data that keeps on growing exponentially with time.
2. Open source framework like **Hadoop** was essential for the growth of Big Data.	
3. **Unstructured data** refers to data that lacks any specific form or structure.
4. **Predictive analytics** is a statistical method that utilizes algorithms and machine learning to identify trends in data and predict future behaviour.	
5. Matrix and algebra operations can be performed using **Map Reduce**.
6. Matrix Multiplication can be done by **cascade** of two Map Reduce operations.
7. **Reduce** phase combines all values associated with an identical key.
8. Once shuffling is done, the output is sent to the **sorting** phase where all the (key, value) pairs are sorted automatically.
9. IoT function Block provides the system capabilities for **sensing**, **actuation monitoring**, and **control functions**.
10. Authentication, authorization, and context integrity come under **Security** systems.
11. Request-Respond, Publish–Subscribe are IoT **communication** models.
12. IoT is enabled by several technologies including **Wireless Sensor Networks (WSN)**, **Cloud Computing**, and **Big Data Analysis**.
13. A RESTful web service is a web API implemented using **HTTP** and **REST** principles.
14. IoT applications provide an interface that users can use to control and monitor various aspects of **IoT Systems**.
15. REST services operate over **HTTP**, and each request is independent of each other.
16. **Apache Spark** is an open-source big data processing framework built around speed, ease of use, and sophisticated analysis, developed at UC Berkeley’s AMP Lab.

## Group B

### 2(a) What do you mean by Big data how is it different from the traditional data ? 

**Big Data** refers to extremely large and complex datasets that cannot be handled by traditional data processing tools. It is typically defined by the **3Vs**:

- **Volume** – Huge amount of data (terabytes to zettabytes)
- **Velocity** – High speed of data generation and processing
- **Variety** – Multiple types of data (structured, semi-structured, unstructured)

### Difference Between Big Data and Traditional Data

| Feature             | Traditional Data                         | Big Data                                         |
|--------------------|-------------------------------------------|--------------------------------------------------|
| **Volume**         | MBs to GBs                                | TBs to PBs and beyond                            |
| **Variety**        | Mostly structured                         | Structured, semi-structured, unstructured        |
| **Velocity**       | Batch processing                          | Real-time or near real-time processing           |
| **Tools**          | SQL, Excel                                | Hadoop, Spark, NoSQL, Kafka                      |
| **Storage**        | Centralized databases                     | Distributed file systems (HDFS, S3, etc.)        |
| **Scalability**    | Vertical (limited)                        | Horizontal (distributed clusters)                |

---

### 2(b) Write all the big data Frameworks. 

Here are popular frameworks used in Big Data:

- **Apache Hadoop**
- **Apache Spark**
- **Apache Flink**
- **Apache Storm**
- **Apache Kafka**
- **Apache Hive**
- **Apache Pig**
- **HBase**

---

### 2(c) Describe the tools that are required to handle big data.

Big Data workflows rely on multiple categories of tools:

#### Storage Tools
- **HDFS (Hadoop Distributed File System)**
- **Amazon S3**
- **Google Cloud Storage**
- **MongoDB / Cassandra**

#### Processing & Computation Tools
- **Apache Spark**
- **Apache Flink**
- **Apache Storm**
- **MapReduce**

#### Querying and Analysis Tools
- **Apache Hive**
- **Apache Impala**
- **Apache Drill**
- **Presto**

#### Visualization & BI Tools
- **Tableau**
- **Power BI**
- **Apache Superset**
- **Grafana**

#### Machine Learning & Analytics
- **Apache Mahout**
- **MLlib (Spark)**
- **TensorFlow / PyTorch**

#### Data Ingestion & Orchestration
- **Apache Kafka**
- **Apache NiFi**
- **Apache Airflow**
- **Apache Sqoop**

---

### 3(a) Compare structured data from unstructured and semi structured data 

| Criteria                | Structured Data                           | Semi-Structured Data                       | Unstructured Data                          |
|------------------------|--------------------------------------------|--------------------------------------------|---------------------------------------------|
| **Definition**          | Clearly defined format, stored in tables  | Partial organization with tags/markers     | No predefined format or structure           |
| **Storage**             | RDBMS (e.g., MySQL, PostgreSQL)            | XML, JSON, NoSQL databases                  | Text files, images, videos, PDFs            |
| **Schema**              | Fixed schema                               | Flexible schema                             | No schema                                   |
| **Query Language**      | SQL                                         | XPath, XQuery, custom parsers               | NLP/AI-based parsing or manual methods      |
| **Examples**            | Employee database, bank records            | XML-based config, JSON APIs                 | Social media posts, photos, audio files     |
| **Ease of Analysis**    | High (well-structured)                     | Medium (requires parsing)                   | Low (needs complex processing)              |

---

### 3(b) Explain the software stack of Big data.

The Big Data stack is a layered architecture consisting of the following components:

#### 1. **Data Sources**
- Sensors, Web Logs, Social Media, Mobile Apps, IoT devices

#### 2. **Data Ingestion**
- **Apache Kafka**, **Apache Flume**, **Apache Sqoop**, **Apache NiFi**

#### 3. **Data Storage**
- **HDFS (Hadoop Distributed File System)**
- **Amazon S3**, **Google Cloud Storage**
- **NoSQL Databases**: HBase, Cassandra, MongoDB

#### 4. **Data Processing**
- **Batch Processing**: Apache Hadoop (MapReduce), Apache Spark
- **Stream Processing**: Apache Flink, Apache Storm, Kafka Streams

#### 5. **Data Querying & Analysis**
- **SQL-on-Hadoop**: Apache Hive, Apache Impala, Presto
- **Interactive Tools**: Apache Drill

#### 6. **Data Orchestration**
- **Workflow Management**: Apache Airflow, Apache Oozie

#### 7. **Visualization & BI**
- **Tableau**, **Power BI**, **Apache Superset**, **Grafana**

---

### 3(c)Describe the physical organization of computer nodes in distributed file systems.

In a distributed file system like HDFS, data is stored across multiple nodes organized into a cluster.

#### Components of a Distributed File System:

- **NameNode (Master)**:
  - Manages metadata and namespace
  - Keeps track of file directory structure and block locations

- **DataNodes (Workers)**:
  - Store actual data blocks
  - Perform read/write operations as instructed by the NameNode

#### Data Distribution:
- Files are split into fixed-size blocks (e.g., 128 MB)
- Each block is replicated (default: 3 copies) across different DataNodes for fault tolerance

#### Node Organization:
- Nodes are often grouped in **racks**
- **Rack awareness** policy ensures data is replicated across racks to improve fault tolerance and reduce data loss risk
- Communication between nodes occurs over a high-speed network (Gigabit or 10G Ethernet)

#### Fault Tolerance:
- If a DataNode fails, the system uses replicated blocks from other nodes
- NameNode monitors DataNode health using heartbeat signals

#### Visual Representation
![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/Namenode-and-Datanode.png?raw=true)
---

### 4(a) Compare the role of different Distributed File Systems .

| Distributed File System | Role & Features                                                                 | Use Cases                                   |
|-------------------------|----------------------------------------------------------------------------------|---------------------------------------------|
| **HDFS (Hadoop DFS)**   | Fault-tolerant, block-based storage system designed for large-scale batch data   | Hadoop ecosystem, Big Data analytics        |
| **Google File System (GFS)** | Proprietary system by Google, fault-tolerant, optimized for Google's workload | Web indexing, Search engine backend         |
| **Amazon S3**           | Object-based storage with scalability, durability, and pay-as-you-go pricing     | Cloud backups, Web hosting, Data lakes      |
| **Azure Data Lake Storage** | Optimized for analytics workloads with hierarchical namespace                 | Azure Big Data solutions                    |

---

### 4(b) How Are Large File System Organizations Dealt?

Large file systems face challenges like managing billions of files, ensuring high availability, and handling large file sizes. The following strategies are used:

#### Strategies to Handle Large File Systems:

- **Block-Based Storage**: Files are split into fixed-size blocks (e.g., 64MB or 128MB in HDFS) for distributed storage.
- **Replication**: Blocks are replicated across nodes to ensure fault tolerance and high availability.
- **Metadata Management**: A centralized or distributed metadata service (e.g., NameNode in HDFS) keeps track of file locations and structure.
- **Namespace Scaling**: Use of partitioned namespaces or hierarchical file structures to manage large directories efficiently.
- **Compression and Encoding**: Reduces storage footprint and increases I/O efficiency.
- **Striping**: Data is striped across disks or nodes for parallel access and faster read/write operations.
- **Caching and Buffering**: Frequently accessed files are cached to reduce latency.

---

### 4(c) Explain the Map Reduce Paradigm. How can it be applied?

**MapReduce** is a programming model for processing and generating large datasets with a parallel, distributed algorithm on a cluster.

#### Two Main Phases:

1. **Map Phase**:
   - Input data is split into `<key, value>` pairs.
   - Each pair is processed independently to produce intermediate `<key, value>` pairs.
   - Example: Counting words → input: `"cat cat dog"`, output: `("cat", 1), ("cat", 1), ("dog", 1)`

2. **Reduce Phase**:
   - Intermediate pairs are grouped by key and reduced to a single output per key.
   - Example: `("cat", [1,1])` → `("cat", 2)`

#### How It Works:

- Input files are divided into chunks and processed in parallel by **mappers**.
- Intermediate results are **shuffled and sorted**.
- Results are then processed by **reducers** to generate final output.

#### Applications of MapReduce:

- **Word Count**: Counting word occurrences in large documents
- **Log Analysis**: Processing server logs at scale
- **Inverted Indexing**: Search engine indexing
- **Data Mining**: Large-scale pattern detection
- **ETL Pipelines**: Data transformation and loading tasks

---

### 5(a) Comparison: Role of HDFS, YARN, and HBase

| Component | Full Form                          | Role in Hadoop Ecosystem                                                                 |
|-----------|------------------------------------|-------------------------------------------------------------------------------------------|
| **HDFS**  | Hadoop Distributed File System     | A distributed storage system that stores large files across multiple machines. It ensures fault tolerance, scalability, and high throughput for large data sets. |
| **YARN**  | Yet Another Resource Negotiator    | Acts as the **resource manager** of Hadoop. It allocates system resources and schedules jobs among applications. Enables multi-tenancy and scalability. |
| **HBase** | Hadoop Database                    | A **NoSQL, column-oriented** distributed database that runs on top of HDFS. It supports random, real-time read/write access to large datasets. |

---

### 5(b) Functions of Combiners

**Combiners** are mini-reducers used in the **MapReduce** framework to optimize performance by minimizing data transfer between the Map and Reduce phases.

#### Key Functions:
- Operate **after the Map phase** and **before the Reduce phase**.
- Perform **local aggregation** of intermediate outputs.
- Reduce the volume of data that is shuffled and sorted across the network.
- Useful for operations like **sum**, **count**, **min**, **max**, etc.

> Example: In a word count task, a combiner can sum word counts locally on each mapper before sending to the reducer.

---

### 5(c) Comparison Parameters: SQL vs NoSQL

| Parameter             | SQL (Relational DBs)                  | NoSQL (Non-Relational DBs)                    |
|-----------------------|----------------------------------------|-----------------------------------------------|
| **Data Model**        | Tabular (rows and columns)             | Key-Value, Document, Column-Family, Graph     |
| **Schema**            | Fixed, predefined schema               | Dynamic, flexible schema                      |
| **Scalability**       | Vertical (scale-up)                    | Horizontal (scale-out)                        |
| **Transactions**      | ACID compliant                         | BASE properties (eventual consistency)        |
| **Examples**          | MySQL, PostgreSQL, Oracle              | MongoDB, Cassandra, Redis           |
| **Best For**          | Structured data, complex joins         | Unstructured/semi-structured data, large scale |
| **Query Language**    | SQL (Structured Query Language)        | Varies: JSON-based queries, APIs              |
| **Data Integrity**    | High                                    | Depends on DB type and configuration          |

---

### 6(a) How Do Combiners Work? Explain with a diagram.

**Combiners** are optional components in the **MapReduce** framework that perform **local aggregation** of intermediate map outputs before they are sent across the network to the reducers.

![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/mapreduce.png?raw=true)

---

### 6(b) Advantages of Combiners
- **Minimize Data Transfer** : Reduces the amount of data shuffled between Mappers and Reducers.
- **Improve Performance** : Speeds up MapReduce jobs by decreasing network congestion.
- **Local Optimization** : Performs computation locally before sending to the reducer.
- **Effective for Aggregations** : Especially useful in word count, sum, average, and min/max operations.
- **Better Resource Utilization** : Reduces I/O and network bandwidth usage.

---

### 6(c) Components of MapReduce Architecture
Component	Description
1. **Input Data** :	Raw data split into blocks (input splits) and processed in parallel
2. **Mapper** :	Processes each input split to generate intermediate key-value pairs
3. **Combiner	(Optional)**: Aggregates mapper output locally before shuffle
4. **Partitioner** :	Assigns intermediate keys to reducers based on a partitioning logic
5. **Shuffle & Sort** :	Transfers intermediate data to reducers and sorts by key
6. **Reducer** :	Aggregates and processes grouped intermediate data to generate final output
7. **Output Format** :	Writes the final output to HDFS or another file system

### 7(a) Briefly explain the method to find relative frequency and cumulative relative frequency using numerical example .

#### Definitions:
- **Relative Frequency**: Frequency of a value divided by total number of values.
- **Cumulative Relative Frequency**: Running total of relative frequencies up to a certain point.

### Example:

| Value | Frequency |
|-------|-----------|
| 10    | 2         |
| 20    | 3         |
| 30    | 5         |

**Step 1: Total Frequency** = 2 + 3 + 5 = 10

**Step 2: Calculate Relative Frequency**

| Value | Frequency | Relative Frequency (f/N) |
|-------|-----------|---------------------------|
| 10    | 2         | 2/10 = 0.2                 |
| 20    | 3         | 3/10 = 0.3                 |
| 30    | 5         | 5/10 = 0.5                 |

**Step 3: Cumulative Relative Frequency**

| Value | Cumulative Relative Frequency |
|-------|-------------------------------|
| 10    | 0.2                           |
| 20    | 0.2 + 0.3 = 0.5               |
| 30    | 0.5 + 0.5 = 1.0               |

---

### 7(b) What is Relative Frequency?

**Relative Frequency** is the ratio of the number of times a specific value occurs to the total number of observations.

#### Formula:
**Relative Frequency = Frequency of a value / Total number of values**

#### Example:
If a value appears **4 times** in a dataset of **20 values**, its relative frequency is:

**Relative Frequency = 4 / 20 = 0.2**

---

### 7(c) Compute the Mean using the Mapper class and Reducer Class .
```python
# Mapper Class
class Mapper:
    def map(self, values):
        for value in values:
            yield ("sum", value)
            yield ("count", 1)

# Reducer Class
class Reducer:
    def reduce(self, mapped_data):
        result = {"sum": 0, "count": 0}
        for key, value in mapped_data:
            result[key] += value

        mean = result["sum"] / result["count"] if result["count"] != 0 else 0
        return mean

# Example usage
if __name__ == "__main__":
    data = [10, 20, 30, 40, 50]
    mapper = Mapper()
    reducer = Reducer()

    # Simulate map phase
    mapped_output = list(mapper.map(data))

    # Group by keys
    shuffled = {}
    for key, value in mapped_output:
        shuffled.setdefault(key, []).append(value)

    # Flatten grouped data for reduce phase
    flattened = [(k, v) for k, values in shuffled.items() for v in values]

    # Reduce phase
    mean_result = reducer.reduce(flattened)
    print("Mean:", mean_result)
```
---

### 8(a) Compare the role of Pair and Strips . Which framework are they associated?

#### Associated Framework:
Both **Pair** and **Stripes** approaches are used in **Hadoop MapReduce** for **co-occurrence matrix computation**, especially in Natural Language Processing (NLP) and text mining tasks.

---

#### Comparison Table

| Aspect                  | **Pair Approach**                                      | **Stripes Approach**                                   |
|-------------------------|--------------------------------------------------------|--------------------------------------------------------|
| **Key Format**          | Uses a **composite key** like `(word1, word2)`         | Uses a **single key** like `word1`, value is a hashmap |
| **Emitted Data Size**   | More key-value pairs (e.g., one for every pair)         | Fewer pairs; emits one map (dictionary) per word       |
| **Efficiency**          | Higher shuffle and sort cost due to more key-value pairs | More efficient for frequent words (lower network I/O) |
| **Memory Usage**        | Low — no need to store hashmaps                         | Higher — maintains an in-memory hashmap                |
| **Reducer Complexity**  | Simple, since input keys are already granular           | Complex — needs to merge dictionaries                  |
| **Example Output**      | `(word1, word2) → 1`                                    | `word1 → {word2: count, word3: count}`                 |
| **Use Case**            | Suitable when memory is constrained                     | Suitable when memory is sufficient for aggregation     |

---

### 8(b) Explain the function of Combiners. 

**Combiners** are mini-reducers used in the **MapReduce** framework to optimize performance by minimizing data transfer between the Map and Reduce phases.

#### Key Functions:
- Operate **after the Map phase** and **before the Reduce phase**.
- Perform **local aggregation** of intermediate outputs.
- Reduce the volume of data that is shuffled and sorted across the network.
- Useful for operations like **sum**, **count**, **min**, **max**, etc.

> Example: In a word count task, a combiner can sum word counts locally on each mapper before sending to the reducer.

---

### 8(c) Describe briefly the components of Map Reduce Architecture.

Component	Description
1. **Input Data** :	Raw data split into blocks (input splits) and processed in parallel
2. **Mapper** :	Processes each input split to generate intermediate key-value pairs
3. **Combiner	(Optional)**: Aggregates mapper output locally before shuffle
4. **Partitioner** :	Assigns intermediate keys to reducers based on a partitioning logic
5. **Shuffle & Sort** :	Transfers intermediate data to reducers and sorts by key
6. **Reducer** :	Aggregates and processes grouped intermediate data to generate final output
7. **Output Format** :	Writes the final output to HDFS or another file system

--- 

### 9(a) Compare the role of Mapper and Reducer function .

| Feature              | **Mapper**                                                    | **Reducer**                                                  |
|----------------------|---------------------------------------------------------------|--------------------------------------------------------------|
| **Position in Flow** | Executes first, processes input data                          | Executes after Shuffle & Sort, processes intermediate data   |
| **Input**            | Raw input split (e.g., lines of text, key-value records)      | Grouped key and list of associated values                    |
| **Output**           | Intermediate key-value pairs                                  | Final output key-value pairs                                 |
| **Purpose**          | Transforms and filters raw data                               | Aggregates and summarizes intermediate data                  |
| **Parallelism**      | Highly parallel (1 per input split)                           | Limited by number of output keys                             |
| **Examples**         | Tokenizing, Filtering, Preprocessing                          | Counting, Summing, Reducing lists of values                  |
| **Stateless/Stateful**| Typically stateless                                          | Can perform stateful aggregation                             |

---

### 9(b) What are the tasks for which Mapper and Reducer classes can be written? Give an Example.

#### Typical Use Cases

| Task                          | Mapper Role                                      | Reducer Role                                       |
|-------------------------------|--------------------------------------------------|----------------------------------------------------|
| **Word Count**                | Emits each word with count `1`                  | Sums counts for each word                          |
| **Sorting**                   | Emits sortable key-value pairs                   | Aggregates or formats sorted keys                  |
| **Inverted Indexing**         | Emits (word, documentID)                         | Aggregates document IDs per word                   |
| **Log Analysis**              | Emits (IP, 1) or (StatusCode, 1)                 | Counts occurrences of IPs or status codes          |
| **Join Operations**           | Emits key and source-tagged value from both sets | Merges/join records based on key                   |

#### Word Count Example:

```python
# Mapper emits:
("apple", 1)
("banana", 1)
("apple", 1)

# Reducer receives:
("apple", [1, 1]) → ("apple", 2)
("banana", [1])   → ("banana", 1)
```

### 9(c) Describe briefly the functions Distributed file systems.

A Distributed File System (DFS) like HDFS or GFS allows storage and access to data across multiple machines as if it were on a single file system.

#### Key Functions:

- **Data Distribution** : Splits large files into blocks and distributes across nodes.
- **Fault Tolerance** : Replicates data blocks across multiple nodes to avoid data loss.
- **Scalability** : Easily scaled horizontally by adding more nodes.
- **High Availability** : Ensures data is accessible even if some nodes fail.
- **Parallel Access** : Multiple clients can read/write data concurrently.
- **Metadata Management** : Maintains a namespace and file-to-block mapping via a central master (e.g., NameNode in HDFS).
- **Security and Access Control** : Manages user permissions, authentication, and secure access.

---

### 10(a) Difference Between Traditional and Distributed File Systems

| Feature                          | **Traditional File System**                                     | **Distributed File System (DFS)**                             |
|----------------------------------|------------------------------------------------------------------|----------------------------------------------------------------|
| **Storage Location**             | Stores data on a **single physical machine**                    | Stores data **across multiple machines** (nodes)               |
| **Scalability**                  | Limited to the capacity of one system                           | Scales horizontally by adding nodes                           |
| **Fault Tolerance**              | If the machine fails, data can be lost                          | Uses **replication** to ensure fault tolerance                 |
| **Performance**                  | Dependent on a single system’s I/O                              | Parallel data access improves performance                     |
| **Data Sharing**                 | Hard to share across systems                                    | Provides transparent access from multiple systems             |
| **Examples**                     | NTFS, FAT32, ext3                                               | HDFS (Hadoop), GFS (Google), Ceph                              |
| **Use Case**                     | Personal computers, small-scale servers                         | Big Data, Cloud Storage, Distributed Computing                 |
| **Data Access**                  | Accessed locally by the same system                             | Can be accessed by multiple systems over a network             |
| **Management**                   | Simple file hierarchy and metadata management                   | More complex with block-level metadata and node management     |

---

### 10(b) Write the basic mapper and reducer function for calculating mean.   

```python
# Mapper Class
class Mapper:
    def map(self, values):
        for value in values:
            yield ("sum", value)
            yield ("count", 1)

# Reducer Class
class Reducer:
    def reduce(self, mapped_data):
        result = {"sum": 0, "count": 0}
        for key, value in mapped_data:
            result[key] += value

        mean = result["sum"] / result["count"] if result["count"] != 0 else 0
        return mean

# Example usage
if __name__ == "__main__":
    data = [10, 20, 30, 40, 50]
    mapper = Mapper()
    reducer = Reducer()

    # Simulate map phase
    mapped_output = list(mapper.map(data))

    # Group by keys
    shuffled = {}
    for key, value in mapped_output:
        shuffled.setdefault(key, []).append(value)

    # Flatten grouped data for reduce phase
    flattened = [(k, v) for k, values in shuffled.items() for v in values]

    # Reduce phase
    mean_result = reducer.reduce(flattened)
    print("Mean:", mean_result)
```
---

### 10(d) Describe briefly the Map-Reduce architecture. Give example of Framework following it.

Component	Description
1. **Input Data** :	Raw data split into blocks (input splits) and processed in parallel
2. **Mapper** :	Processes each input split to generate intermediate key-value pairs
3. **Combiner	(Optional)**: Aggregates mapper output locally before shuffle
4. **Partitioner** :	Assigns intermediate keys to reducers based on a partitioning logic
5. **Shuffle & Sort** :	Transfers intermediate data to reducers and sorts by key
6. **Reducer** :	Aggregates and processes grouped intermediate data to generate final output
7. **Output Format** :	Writes the final output to HDFS or another file system

| Framework       | Description                                                                 |
|-----------------|-----------------------------------------------------------------------------|
| **Apache Hadoop** | The most popular open-source implementation of MapReduce. Uses HDFS + YARN. |
| **Apache Hive**   | SQL-like querying on large datasets, translates queries into MapReduce jobs. |
| **Apache Pig**    | High-level platform for data transformation; internally uses MapReduce.    |

---

### 11(a) Compare the traditional file system from distributed file systems.

| Feature                          | **Traditional File System**                                     | **Distributed File System (DFS)**                             |
|----------------------------------|------------------------------------------------------------------|----------------------------------------------------------------|
| **Storage Location**             | Stores data on a **single physical machine**                    | Stores data **across multiple machines** (nodes)               |
| **Scalability**                  | Limited to the capacity of one system                           | Scales horizontally by adding nodes                           |
| **Fault Tolerance**              | If the machine fails, data can be lost                          | Uses **replication** to ensure fault tolerance                 |
| **Performance**                  | Dependent on a single system’s I/O                              | Parallel data access improves performance                     |
| **Data Sharing**                 | Hard to share across systems                                    | Provides transparent access from multiple systems             |
| **Examples**                     | NTFS, FAT32, ext3                                               | HDFS (Hadoop), GFS (Google), Ceph                              |
| **Use Case**                     | Personal computers, small-scale servers                         | Big Data, Cloud Storage, Distributed Computing                 |
| **Data Access**                  | Accessed locally by the same system                             | Can be accessed by multiple systems over a network             |
| **Management**                   | Simple file hierarchy and metadata management                   | More complex with block-level metadata and node management     |

---

### 11(b) What do you understand from the Map Reduce Paradigm?

**MapReduce** is a programming model for processing and generating large datasets with a parallel, distributed algorithm on a cluster.

#### Two Main Phases:

1. **Map Phase**:
   - Input data is split into `<key, value>` pairs.
   - Each pair is processed independently to produce intermediate `<key, value>` pairs.
   - Example: Counting words → input: `"cat cat dog"`, output: `("cat", 1), ("cat", 1), ("dog", 1)`

2. **Reduce Phase**:
   - Intermediate pairs are grouped by key and reduced to a single output per key.
   - Example: `("cat", [1,1])` → `("cat", 2)`

#### How It Works:

- Input files are divided into chunks and processed in parallel by **mappers**.
- Intermediate results are **shuffled and sorted**.
- Results are then processed by **reducers** to generate final output.

--- 

### 11(c) Name the Big data Frameworks and their area of usage.

Big Data workflows rely on multiple categories of tools:

#### Storage Tools
- **HDFS (Hadoop Distributed File System)**
- **Amazon S3**
- **Google Cloud Storage**
- **MongoDB / Cassandra**

#### Processing & Computation Tools
- **Apache Spark**
- **Apache Flink**
- **Apache Storm**
- **MapReduce**

#### Querying and Analysis Tools
- **Apache Hive**
- **Apache Impala**
- **Apache Drill**
- **Presto**

#### Visualization & BI Tools
- **Tableau**
- **Power BI**
- **Apache Superset**
- **Grafana**

#### Machine Learning & Analytics
- **Apache Mahout**
- **MLlib (Spark)**
- **TensorFlow / PyTorch**

#### Data Ingestion & Orchestration
- **Apache Kafka**
- **Apache NiFi**
- **Apache Airflow**
- **Apache Sqoop**

---

### 12(a) What is the Importance of Big Data Analysis?

Big Data Analysis plays a critical role in modern data-driven decision-making across industries. Here's why it's important:

- **Improved Decision Making**: Enables organizations to make more informed, real-time decisions.
- **Customer Insights**: Helps in understanding customer behavior and preferences.
- **Cost Efficiency**: Identifies inefficiencies and areas to reduce operational costs.
- **Innovation**: Drives innovation by revealing hidden patterns and trends.
- **Operational Efficiency**: Automates and optimizes business processes.
- **Risk Management**: Detects fraud and security breaches through pattern analysis.

---

### 12(b) Describe Any Distributed File System

#### Hadoop Distributed File System

In a distributed file system like HDFS, data is stored across multiple nodes organized into a cluster.

#### Components of a Distributed File System:

- **NameNode (Master)**:
  - Manages metadata and namespace
  - Keeps track of file directory structure and block locations

- **DataNodes (Workers)**:
  - Store actual data blocks
  - Perform read/write operations as instructed by the NameNode

#### Data Distribution:
- Files are split into fixed-size blocks (e.g., 128 MB)
- Each block is replicated (default: 3 copies) across different DataNodes for fault tolerance

#### Node Organization:
- Nodes are often grouped in **racks**
- **Rack awareness** policy ensures data is replicated across racks to improve fault tolerance and reduce data loss risk
- Communication between nodes occurs over a high-speed network (Gigabit or 10G Ethernet)

#### Fault Tolerance:
- If a DataNode fails, the system uses replicated blocks from other nodes
- NameNode monitors DataNode health using heartbeat signals

#### Visual Representation

![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/Namenode-and-Datanode.png?raw=true)

---

### 12(c) Write the functions of mapper and reducer class.

Below is an example of using Python to simulate MapReduce for computing **word count**:

#### Mapper and Reducer Function

```python
from collections import defaultdict
def mapper(text):
    mapped_output = []
    for line in text.strip().split("\n"):
        for word in line.strip().split():
            mapped_output.append((word.lower(), 1))
    return mapped_output
def reducer(mapped_data):
    reduced_output = defaultdict(int)
    for word, count in mapped_data:
        reduced_output[word] += count
    return dict(reduced_output)
if __name__ == "__main__":
    input_text = "big data is big\nbig data is powerful"
    mapped = mapper(input_text)
    result = reducer(mapped)
    print(result)
```

**Output**
`{'big': 3, 'data': 2, 'is': 2, 'powerful': 1}`

---

### 13(a) What is the role of distributed file systems in terms of processing Big Data ?

Distributed File Systems play a **critical role** in enabling the storage, management, and processing of Big Data:

| Functionality                  | Role in Big Data Processing                                                                 |
|-------------------------------|----------------------------------------------------------------------------------------------|
| **Data Distribution**         | Splits large datasets into smaller blocks and stores them across multiple nodes.            |
| **Fault Tolerance**           | Automatically replicates data blocks to prevent data loss due to hardware failure.          |
| **Scalability**               | Can scale horizontally by adding more nodes to accommodate growing data volumes.           |
| **Parallel Processing**       | Supports parallel data access and computation (e.g., with MapReduce, Spark, etc.).          |
| **High Throughput**           | Optimized for large batch operations and streaming data access.                            |
| **Data Locality Optimization**| Moves computation closer to where the data resides to reduce network overhead.              |
| **Integration with Ecosystems**| Seamlessly integrates with processing frameworks like Hadoop, Spark, Hive, etc.            |

---

### 13(b) Explain Combiners. Design a basic Map Reduce Algorithm.

**Combiners** are optional components in the **MapReduce** framework that perform **local aggregation** of intermediate map outputs before they are sent across the network to the reducers.

![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/mapreduce.png?raw=true)

Below is an example of using Python to simulate MapReduce for computing **word count**:

#### Mapper and Reducer Function

```python
from collections import defaultdict
def mapper(text):
    mapped_output = []
    for line in text.strip().split("\n"):
        for word in line.strip().split():
            mapped_output.append((word.lower(), 1))
    return mapped_output
def reducer(mapped_data):
    reduced_output = defaultdict(int)
    for word, count in mapped_data:
        reduced_output[word] += count
    return dict(reduced_output)
if __name__ == "__main__":
    input_text = "big data is big\nbig data is powerful"
    mapped = mapper(input_text)
    result = reducer(mapped)
    print(result)
```

**Output**
`{'big': 3, 'data': 2, 'is': 2, 'powerful': 1}`

---

## Group C

### 14(a) Explain Map Reduce and It’s Phases with the help of a Block Diagram . 	

Component	Description
1. **Input Data** :	Raw data split into blocks (input splits) and processed in parallel
2. **Mapper** :	Processes each input split to generate intermediate key-value pairs
3. **Combiner	(Optional)**: Aggregates mapper output locally before shuffle
4. **Shuffle & Sort** :	Transfers intermediate data to reducers and sorts by key
5. **Reducer** :	Aggregates and processes grouped intermediate data to generate final output
6. **Output Format** :	Writes the final output to HDFS or another file system

**Block Diagram**

![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/mapreduce.png?raw=true)

---

### 14(b) Give a Numerical Example of Map Reduce Function to generate output Matrix . 

#### Input Matrices

#### Matrix A (2x3)
A = [ [1, 2, 3], [4, 5, 6] ]

#### Matrix B (3x2)
B = [ [7, 8], [9, 10], [11, 12] ]

#### 1. Map Phase
For each A[i][j], emit key = (i,k) and value = A[i][j]  
For each B[j][k], emit key = (i,k) and value = B[j][k]  
→ Combine A and B values with matching keys (i,k)

#### 2. Shuffle Phase
Group values by key (i,k) = output cell position in result matrix C.

Example for key (0,0):
A row 0: 1, 2, 3 B col 0: 7, 9, 11

→ Multiply and sum: 1×7 + 2×9 + 3×11 = 58

#### 3. Reduce Phase

For each key (i,k), compute:
C[i][k] = A[i][0]*B[0][k] + A[i][1]*B[1][k] + A[i][2]*B[2][k]

#### Final Output Matrix C (2x2)
C = [ [58, 64], [139, 154] ]

---
### 14(c) Write a Algorithm for Mapper and Reducer Function for word count.

Below is an example of using Python to simulate MapReduce for computing **word count**:

#### Mapper and Reducer Function

```python
from collections import defaultdict
def mapper(text):
    mapped_output = []
    for line in text.strip().split("\n"):
        for word in line.strip().split():
            mapped_output.append((word.lower(), 1))
    return mapped_output
def reducer(mapped_data):
    reduced_output = defaultdict(int)
    for word, count in mapped_data:
        reduced_output[word] += count
    return dict(reduced_output)
if __name__ == "__main__":
    input_text = "big data is big\nbig data is powerful"
    mapped = mapper(input_text)
    result = reducer(mapped)
    print(result)
```

**Output**
`{'big': 3, 'data': 2, 'is': 2, 'powerful': 1}`

### 15(a) Compare the role of centralized control  in traditional and distributed databases.

| Aspect                | Traditional Databases               | Distributed Databases               |
|-----------------------|-------------------------------------|-------------------------------------|
| **Control Location**  | Single central server               | Decentralized across nodes          |
| **Decision Making**   | Centralized authority               | Consensus-based or partitioned      |
| **Failure Impact**    | Single point of failure             | Fault-tolerant (partial failures)   |
| **Scalability**       | Vertical scaling only               | Horizontal scaling possible         |
| **Consistency Model** | Strong consistency                 | Eventual consistency common         |
| **Example**           | MySQL on one server                 | Cassandra cluster                   |

---

### 15(b) Explain Matrix Vector Multiplication by Map Reduce. 

Multiply a matrix **A (m×n)** with a vector **v (n×1)** using the **MapReduce paradigm**.

---

**Input Example**
- Matrix A (3x3): A = [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ]
- Vector v (3x1): v = [1, 2, 3]

#### 1. **Map Phase**
For each element A[i][j], emit:
Key: i Value: A[i][j] * v[j]

**Mapper Output Example**:
- A[0][0] * v[0] = 1 * 1 → emit (0, 1)
- A[0][1] * v[1] = 2 * 2 → emit (0, 4)
- A[0][2] * v[2] = 3 * 3 → emit (0, 9)
- A[1][0] * v[0] = 4 * 1 → emit (1, 4)

### 2. **Shuffle Phase**
Group values by key (row index):
Key: 0 → [1, 4, 9] Key: 1 → [4, 10, 18] Key: 2 → [7, 16, 27]

### 3. **Reduce Phase**
Sum the values for each key to get the dot product of row i and vector v.

**Reducer Output**:
- Row 0: 1 + 4 + 9 = 14
- Row 1: 4 + 10 + 18 = 32
- Row 2: 7 + 16 + 27 = 50

---

#### Final Output Vector (3x1)
[ 14, 32, 50 ]

---

### 15(c) Explain how Google Page Ranking Algorithm works.

**PageRank** is an algorithm used by Google Search to rank web pages in their search engine results. It measures the importance of each page based on the number and quality of links to it.

- A web page is important **if it is linked to by many other important pages**.
- Not all links are equal — links from highly ranked pages carry more weight.

#### How It Works

1. **Start** with all pages having the same initial rank.
2. For each page, distribute its PageRank equally among all the pages it links to.
3. Update each page’s rank based on incoming links.
4. Repeat the process until the values converge (stabilize).

#### PageRank Formula

For a page `P`:

PR(P) = (1 - d)/N + d * (PR(P1)/L(P1) + PR(P2)/L(P2) + ... + PR(Pn)/L(Pn))

Where:
- `PR(P)` = PageRank of page P  
- `d` = damping factor (typically 0.85)  
- `N` = total number of pages  
- `P1...Pn` = pages linking to P  
- `L(Pi)` = number of outbound links on page Pi  


**Damping Factor**

- The damping factor **d** simulates a random user clicking on links.
- A value of 0.85 means there's an **85% chance** the user clicks a link and **15% chance** they jump to any random page.

#### Applications

- Web search engines  
- Social network analysis  
- Citation networks  
- Recommendation systems

### 16(a) Compare the role of relational algebra operations Selection and Projection .

| Operation   | Description                                                                 | Result                             |
|-------------|-----------------------------------------------------------------------------|------------------------------------|
| **Selection (σ)** | Filters rows based on a given predicate (condition).                       | Subset of rows                     |
| **Projection (π)** | Extracts specific columns (attributes) from a relation, removing duplicates. | Subset of columns  |

---

### 16(b) Given a relation R, partition it’s tuples according to their values .Analyse which aggregation methods are better and explain 2 of them. 

Given a relation **R**, we can **partition** tuples by a specific attribute (e.g., category) using **group-by**.

#### Aggregation Methods:
1. **SUM** – Adds values in each group.
   - Useful for financial or quantitative data.
2. **AVG (Average)** – Calculates mean per group.
   - Ideal for computing average performance, prices, etc.

*Better methods* depend on data type and the question:
- Use `SUM` for totals (e.g., total sales).
- Use `AVG` for performance or trends (e.g., average rating).

| Product | Category    | Sales |
|---------|-------------|-------|
| A       | Electronics | 100   |
| B       | Electronics | 150   |
| C       | Clothing    | 80    |
| D       | Clothing    | 120   |
| E       | Groceries   | 60    |

**Step 1**: Group by Category
Group rows by the Category column:
- Electronics → 100, 150
- Clothing → 80, 120
- Groceries → 60
  
**Step 2** : Apply Aggregation

SUM
- Electronics: 100 + 150 = 250
- Clothing: 80 + 120 = 200
- Groceries: 60

AVG
- Electronics: (100 + 150) / 2 = 125
- Clothing: (80 + 120) / 2 = 100
- Groceries: 60 / 1 = 60

| Category    | Total Sales (SUM) | Avg Sales (AVG) |
|-------------|-------------------|-----------------|
| Electronics | 250               | 125             |
| Clothing    | 200               | 100             |
| Groceries   | 60                | 60              |

---

### 16(c) Find the paths of length two in the Web using the relation Links in the Fig below.
 
| From |	To |
|------|-----------|
| url1 |      url2 |
| url1 |      url3 |
| url2 |      url3 |
| url2 |      url4 |
		Fig1: Relation Links having set of pairs of URL’s 

#### Objective:
Find all `(A, C)` such that:  
`A → B → C`  
(i.e., 2 hops)

#### Join:
Perform a **self join** on `Links(A, B)` and `Links(B, C)`:

```text
A       B       C
---------------------
url1    url2    url3
url1    url2    url4
```

Output Paths of Length 2:
- (url1, url3)
- (url1, url4)

### 16(d)Explain grouping and aggregation by Map-Reduce where there is one grouping attribute and one aggregation, where Map will perform the grouping and Reduce does the aggregation.

Example:
Sales data: (Region, Revenue)

Map Phase:
Input: (Region, Revenue)

Emit: key = Region, value = Revenue

#### Mapper output
- ("North", 100)
- ("South", 200)
- ("North", 150)
#### Reduce Phase:
- Key: Region
- Values: List of Revenue
- Aggregate (e.g., SUM):
#### Reducer computes:
- North → 100 + 150 = 250
- South → 200
#### Final Output:
- ("North", 250)
- ("South", 200)

### 17(a) Discuss the relational –algebra operations (a) Selection (b) Projection
#### (a) Selection (σ)
**Definition:**  
Selection is a unary operation in relational algebra that retrieves tuples (rows) from a relation (table) that satisfy a given predicate (condition).

**Notation:**  
σ<sub>condition</sub>(Relation)

**Example:**  
To select employees with salary greater than 50,000:  
σ<sub>salary > 50000</sub>(Employee)

**Properties:**
- Does not change the number of attributes (columns).
- Reduces the number of tuples (rows).

---

### (b) Projection (π)
**Definition:**  
Projection is a unary operation that retrieves specific columns from a relation, eliminating duplicates.

**Notation:**  
π<sub>attribute_list</sub>(Relation)

**Example:**  
To retrieve only the names and salaries of employees:  
π<sub>name, salary</sub>(Employee)

**Properties:**
- Reduces the number of attributes (columns).
- Removes duplicate rows automatically (since relations are sets).

---

### Q17(b) A directed graph whose arcs are represented by the relation E(X,Y), We wish to compute the paths relation  P(X,Y),where there is a path of length 1 or more from node X to node Y . Write down a simple recursive algorithm to Map-Reduce.

**Problem:**  
Given a directed graph with edges represented by the relation `E(X, Y)`, compute the paths relation `P(X, Y)` such that there is a path from node `X` to node `Y` of length ≥ 1.

**Recursive Algorithm using MapReduce:**

```python
P = E
while True:
    # Join P with E to find new paths (P1)
    P1 = {(x, z) for (x, y1) in P for (y2, z) in E if y1 == y2}

    # Union of previous paths and new paths
    P_new = P.union(P1)

    # If no new paths were added, break
    if P_new == P:
        break

    # Update P with new paths
    P = P_new
```

**Output: Final P contains all reachable (X, Y) pairs**

## 18(a) Given a relation R, partition it’s tuples according to their values .Analyse which aggregation methods are better and explain 2 of them.

Given a relation **R**, we can **partition** tuples by a specific attribute (e.g., category) using **group-by**.

#### Aggregation Methods:
1. **SUM** – Adds values in each group.
   - Useful for financial or quantitative data.
2. **AVG (Average)** – Calculates mean per group.
   - Ideal for computing average performance, prices, etc.

*Better methods* depend on data type and the question:
- Use `SUM` for totals (e.g., total sales).
- Use `AVG` for performance or trends (e.g., average rating).

| Product | Category    | Sales |
|---------|-------------|-------|
| A       | Electronics | 100   |
| B       | Electronics | 150   |
| C       | Clothing    | 80    |
| D       | Clothing    | 120   |
| E       | Groceries   | 60    |

**Step 1**: Group by Category
Group rows by the Category column:
- Electronics → 100, 150
- Clothing → 80, 120
- Groceries → 60
  
**Step 2** : Apply Aggregation

SUM
- Electronics: 100 + 150 = 250
- Clothing: 80 + 120 = 200
- Groceries: 60

AVG
- Electronics: (100 + 150) / 2 = 125
- Clothing: (80 + 120) / 2 = 100
- Groceries: 60 / 1 = 60

| Category    | Total Sales (SUM) | Avg Sales (AVG) |
|-------------|-------------------|-----------------|
| Electronics | 250               | 125             |
| Clothing    | 200               | 100             |
| Groceries   | 60                | 60              |

---

### 18(b) Discuss the relation algebra operations (a)Union (b) Intersection(c) Difference 

### (a) Union ( ∪ )
**Definition:**  
Union combines all tuples from two relations and eliminates duplicates.

**Notation:**  
R ∪ S

**Conditions for Union:**
- Both relations must be **union-compatible**:
  - Same number of attributes (columns).
  - Corresponding attributes have the same domain.

**Example:**  
To get a list of all students from two different campuses:  
`Student_CampusA ∪ Student_CampusB`

**Result:**  
All unique students from both campuses.

---

### (b) Intersection ( ∩ )
**Definition:**  
Intersection retrieves tuples that are **common** to both relations.

**Notation:**  
R ∩ S

**Conditions for Intersection:**
- Both relations must be **union-compatible**.

**Example:**  
To find students enrolled in both Campus A and Campus B:  
`Student_CampusA ∩ Student_CampusB`

**Result:**  
Only students present in both relations.

---

### (c) Difference ( − )
**Definition:**  
Difference returns tuples that are **in the first relation but not in the second**.

**Notation:**  
R − S

**Conditions for Difference:**
- Both relations must be **union-compatible**.

**Example:**  
To find students enrolled in Campus A but not in Campus B:  
`Student_CampusA − Student_CampusB`

**Result:**  
Only students exclusive to Campus A.

### Summary Table

| Operation     | Symbol | Result                                     |
|---------------|--------|--------------------------------------------|
| Union         | ∪      | All unique tuples from both R and S        |
| Intersection  | ∩      | Tuples common to both R and S              |
| Difference    | −      | Tuples in R that are not in S              |

---

### 18(c)Explain grouping and aggregation by Map-Reduce where there is one grouping attribute and one aggregation where Map will perform the grouping and Reduce does the aggregation.

Example:
Sales data: (Region, Revenue)

Map Phase:
Input: (Region, Revenue)

Emit: key = Region, value = Revenue

#### Mapper output
- ("North", 100)
- ("South", 200)
- ("North", 150)
#### Reduce Phase:
- Key: Region
- Values: List of Revenue
- Aggregate (e.g., SUM):
#### Reducer computes:
- North → 100 + 150 = 250
- South → 200
#### Final Output:
- ("North", 250)
- ("South", 200)

---

### 19(a) Find shortest path Algorithm using Map reduce.

#### Assumptions:
- Graph is stored as adjacency list with weights.
- Initial distances are set to ∞ (infinity), except the **source node** which is 0.


#### MapReduce Approach

**Step 1: Data Format**

Each line in the graph file:
```text
NodeID    Distance    AdjacencyList
------------------------------------
A    	     0    	B:3,C:1
B    	     ∞    	C:1,D:7
C    	     ∞    	D:1
D    	     ∞    	-
```

```python
def mapper(node_id, value):
    # value: (distance, adjacency_list)
    distance, adjacency = value

    # Emit the node as-is
    emit(node_id, (distance, adjacency))

    # Emit distances to neighbors
    for neighbor, weight in adjacency:
        new_distance = distance + weight
        emit(neighbor, new_distance)
def reducer(node_id, values):
    min_distance = ∞
    adjacency = []

    for val in values:
        if is_adjacency_list(val):
            adjacency = val
        else:
            min_distance = min(min_distance, val)

    emit(node_id, (min_distance, adjacency))

```
---

### 19(b) Write a simple Map Reduce Algorithm Design.

Below is an example of using Python to simulate MapReduce for computing **word count**:

#### Mapper and Reducer Function

```python
from collections import defaultdict
def mapper(text):
    mapped_output = []
    for line in text.strip().split("\n"):
        for word in line.strip().split():
            mapped_output.append((word.lower(), 1))
    return mapped_output
def reducer(mapped_data):
    reduced_output = defaultdict(int)
    for word, count in mapped_data:
        reduced_output[word] += count
    return dict(reduced_output)
if __name__ == "__main__":
    input_text = "big data is big\nbig data is powerful"
    mapped = mapper(input_text)
    result = reducer(mapped)
    print(result)
```

**Output**
`{'big': 3, 'data': 2, 'is': 2, 'powerful': 1}`

---

### 20(a)Compare and analyse the role of Map function and reduce Function .

| Feature              | **Mapper**                                                    | **Reducer**                                                  |
|----------------------|---------------------------------------------------------------|--------------------------------------------------------------|
| **Position in Flow** | Executes first, processes input data                          | Executes after Shuffle & Sort, processes intermediate data   |
| **Input**            | Raw input split (e.g., lines of text, key-value records)      | Grouped key and list of associated values                    |
| **Output**           | Intermediate key-value pairs                                  | Final output key-value pairs                                 |
| **Purpose**          | Transforms and filters raw data                               | Aggregates and summarizes intermediate data                  |
| **Parallelism**      | Highly parallel (1 per input split)                           | Limited by number of output keys                             |
| **Examples**         | Tokenizing, Filtering, Preprocessing                          | Counting, Summing, Reducing lists of values                  |
| **Stateless/Stateful**| Typically stateless                                          | Can perform stateful aggregation                             |

---
### 20(b) Discuss relational algebra operation:  Grouping and aggregation  
#### Definition
**Grouping and Aggregation** are extended (non-basic) operations in **relational algebra** used to compute summary statistics on groups of tuples.

#### Syntax
```text
G⟨grouping_attributes⟩  g⟨aggregate_function(attribute)⟩ (Relation)
```

- `G` : Group-by operator.
- `grouping_attributes` : List of attributes to group by.
- `g` : Aggregation (like SUM, COUNT, AVG, MAX, MIN).

Relation: Input relation (table).

Example Relation: Employee

| emp_id | dept | salary |
|--------|------|--------|
| 1	 | HR	| 50000  |
| 2	 | IT	| 60000  |
| 3	 | HR	| 55000  |
| 4	 | IT	| 70000  |

Example Query

Find the average salary per department:
- G⟨dept⟩ g⟨AVG(salary)⟩ (Employee)

Result:

```text
dept	AVG(salary)
HR	52500
IT	65000
```

**Common Aggregation Functions**
```text
Function	Description
COUNT	Counts the number of tuples
SUM	Adds up the values of an attribute
AVG	Calculates average value
MAX	Finds maximum value
MIN	Finds minimum value
```

**Key Notes**
-Grouping helps summarize large data sets based on categories.
- Not part of classical relational algebra — it is an extended operator.
- Supported in SQL as GROUP BY.

### 21(a) How can we perform Computation in Pregel ?
### [(CO3)((Analyse/HOCQ)]
Steps to Perform Computation in Pregel
1. **Initialization**

Each vertex is initialized with:

- A unique **vertex ID**.
- A mutable **value** (state).
- A set of **outgoing edges** to neighbors.

2. **Vertex Computation (Per Superstep)**

**Input:** Messages received during the previous superstep.
**Processing Example (Python-like pseudocode):**

```python
def Compute(messages):
    # Update vertex state using received messages
    for message in messages:
        self.value = process_message(message)
    
    # Send messages to other vertices
    send_message_to(destination_vertex, message_data)
    
    # Optionally halt if no further work is needed
    if should_halt():
        vote_to_halt()
```
**3. Message Passing**

- Messages are sent **asynchronously**.
- Messages are **delivered at the next superstep**.

**Example:**
- Vertex A sends a message to Vertex B during **Superstep S**.
- Vertex B receives and processes the message during **Superstep S+1**.
**4. Termination**
The algorithm stops when:
- **All vertices** have voted to halt.
- **No pending messages** remain in the system.

---
**Example:** Single-Source Shortest Path (SSSP)

1. Initialization

- The **source vertex** sets `distance = 0`.
- **All other vertices** set `distance = ∞`.
Superstep 1 - The source sends initial distances to its neighbors.
2. Subsequent Supersteps
Each vertex:
- Updates its distance if a **shorter path** is received.
- **Halts** if no shorter path is found.

3. Termination
- The algorithm ends when **no more distance updates** occur.

**Use Cases**
1.Graph Algorithms

- **PageRank**
- **Connected Components**
- **Clustering**

2. Distributed Machine Learning

### 21(b) Differentiate between map function and reduce function.
| Feature             | Map Function                                     | Reduce Function                                    |
|---------------------|--------------------------------------------------|----------------------------------------------------|
| **Purpose**          | Transforms input data into intermediate key-value pairs | Aggregates intermediate data into final output      |
| **Input**            | Raw data (e.g., lines of a file, records)       | Key-value pairs from Map output                    |
| **Output**           | Key-value pairs                                 | Reduced key and aggregated value                   |
| **Operation Type**   | Element-wise processing                         | Group-wise aggregation                             |
| **Execution Style**  | Parallelizable per element                      | Parallelizable per key                             |
| **Example Task**     | Tokenizing lines into words                     | Counting frequency of each word                    |
| **Typical Use Case** | Data preprocessing, filtering, or transformation| Summarization, statistics, consolidation           |

### 22(a) What is Pregel and how does it manage failures when implementing recursive algorithms on a computing cluster ?
**Pregel** is a **vertex-centric programming model** developed by Google for **large-scale graph processing** in **distributed systems**. It is designed to efficiently run graph algorithms—especially iterative and recursive ones—across a cluster of machines, using a model called **Bulk Synchronous Parallel (BSP)**.
- **Vertex-centric**: Computation revolves around vertices; each vertex runs the same user-defined `compute()` function.
- **Message passing**: Vertices communicate by sending and receiving messages in discrete **supersteps**.
- **Bulk Synchronous Parallel**: Each superstep includes:
  1. Receiving messages from the previous superstep.
  2. Updating the vertex state.
  3. Sending messages to other vertices for the next superstep.
  4. Optionally voting to halt.
Pregel is built to run on large clusters, where **failures are expected**. It handles failures using a **checkpointing mechanism**:

| Mechanism            | Description                                                                 |
|----------------------|-----------------------------------------------------------------------------|
| **Checkpointing**     | Periodically, the system saves the state of all vertices and messages to disk. |
| **Failure Detection** | If a machine fails, the system restarts the computation **from the last checkpoint**, not from scratch. |
| **Deterministic Execution** | Since supersteps are deterministic, re-running a failed portion will produce the same result. |
| **Master Node**       | Oversees worker nodes and restarts failed workers or reassigns their workload. |

**Recursive Algorithm Implementation in Pregel**

Recursive or iterative graph algorithms (like BFS, PageRank, or SSSP) naturally map to Pregel’s superstep model:

- **Each recursion level** is handled by one **superstep**.
- **State propagation** and convergence occur via message passing.
- **Termination** is decided when no vertex is active and no message is in transit.
- **Graph-based learning algorithms**
### 22(b) Think matrix as a relation M with row number column number and value. So M(I,J,V) with tuples (i,j,mij), N(J,K,W) with tuples (j,k,njk).Perform grouping and aggregation with Map and Reduce function.
Matrix Multiplication using MapReduce.
Given two matrices:
- Matrix M: Represented as tuples `(i, j, mij)` where `i` is row, `j` is column, `mij` is value
- Matrix N: Represented as tuples `(j, k, njk)` where `j` is row, `k` is column, `njk` is value

We want to compute the product P = M × N where each element `pik = Σ(mij * njk)` for all j
**MapReduce Solution**

**1. Map Phase**

**Mapper for Matrix M:**
```python
def map_M(input_tuple):
    i, j, mij = input_tuple
    # Emit (j) as key to join with N
    for k in all_possible_k_values:
        yield (j, ('M', i, k, mij))
```
**2. Shuffle Phase** 
```python
(j, [('M', i1, k1, m_val), ('N', i2, k2, n_val), ...])
```
**3. Reduce Phase**
**Reducer**
```python
def reduce(j, values):
    # Separate M and N values
    M_values = [v for v in values if v[0] == 'M']
    N_values = [v for v in values if v[0] == 'N']
    
    # Compute products for matching i,k pairs
    for (_, i, _, mij) in M_values:
        for (_, _, k, njk) in N_values:
            yield (i, k, mij * njk)
```
**4. Final Aggregation**
A second MapReduce job performs the final aggregation to sum the partial products computed in the previous reduce phase.
**Final Mapper**
```python
def final_map(input_tuple):
    i, k, product = input_tuple
    yield ((i, k), product)
```
**Final Reducer**
```python
def final_reduce(key, values):
    i, k = key
    yield (i, k, sum(values))
```

### 23(a) How are today’s surveillance system controlling using the wireless Video Camera Networks ?
Wireless Video Camera Networks (WVCNs) are used to manage surveillance systems by enabling video feeds from cameras to be transmitted wirelessly, offering flexibility and scalability. Here's a concise breakdown of how they work:

**1. Wireless Communication**
WVCNs rely on wireless technologies like Wi-Fi, 5G, or mesh networks to transmit video data from cameras to monitoring stations without the need for cabling.

**2. Camera Control**
Cameras can be remotely controlled using Pan-Tilt-Zoom (PTZ) functions or through AI-driven automation that tracks movement or detects specific activities, reducing the need for constant human intervention.

**3. Data Transmission**
Video feeds are sent via real-time streaming protocols (RTSP) to central servers or cloud storage. These systems support live video streaming, playback, and even AI-based processing like facial or object recognition.

**4. Centralized Monitoring**
Control is often centralized through a Network Video Recorder (NVR) or cloud platform where footage is monitored and stored. Edge computing can also be used to process data locally, reducing bandwidth and enhancing real-time analysis.

**5. Security**
Encryption, authentication, and secure communication protocols ensure that video data remains protected. This is especially important in large systems where data privacy and access control are critical.

**6. AI Integration**
AI enhances surveillance by enabling features like automatic tracking, event detection, and facial recognition. This can automate many tasks that would traditionally require human intervention.

### 23(b) What is the purpose of Communication cost Model?  
The role of Communication Cost Model are:
**1. Evaluate Performance:**
It helps determine how much time or resources are consumed in sending and receiving data, which can significantly impact the total runtime of a parallel algorithm.

**2. Optimize Algorithms:**
By understanding the communication cost, developers can design algorithms that minimize data transfer, leading to more efficient execution on distributed systems.

**3. Compare Architectures:**
Communication cost models allow performance comparisons across different hardware setups, such as multi-core CPUs, clusters, or GPUs.

**4. Guide Scheduling and Partitioning:**
Helps in deciding how to split data and tasks among nodes to reduce the cost of communication and balance computational loads.

### 23(c) Write a simple recursive algorithm to compute the path P(X,Y)  from node X and Node Y in a directed graph.
Suppose we have a directed graph whose arcs are represented by the relation E(X, Y ), meaning that there is an arc from node X to node Y.  
We wish to compute the paths relation P(X, Y), meaning that there is a path of length 1 or more from node X to node Y. That is, P is the transitive closure  
of E. A simple recursive algorithm to do so is:
1. Start with P(X, Y ) = E(X, Y).
2. While changes to the relation P occur, add to P all tuples in    
   $\pi_{X,Y} \left(P(X, Z) \bowtie P(Z, Y)\right)$

That is, find pairs of nodes X and Y such that for some node Z there is known to be a path from X to Z and also a known path from Z to Y.

### 24(a) What are the future scope of wireless video camera networks in surveillance systems? [First part answered in 23(a)]
The future scope of wireless video camera networks in surveillance systems is significant, driven by advances in connectivity, artificial intelligence, and edge computing. Some key developments include:

1. AI-Powered Analytics: Integration of machine learning for real-time facial recognition, anomaly detection, and behavior prediction will make surveillance more proactive and intelligent.
2. Edge Computing: Processing video data closer to the source (at the edge) will reduce latency, improve response times, and reduce bandwidth usage.
3. 5G and Beyond: High-speed, low-latency wireless communication (like 5G/6G) will enable faster and more reliable video streaming, even in remote or mobile surveillance setups (e.g., drones, vehicles).
4. IoT and Smart City Integration: Wireless cameras will increasingly become part of larger IoT ecosystems, contributing to smart city infrastructure for traffic monitoring, public safety, and environmental sensing.

### 24(b) What are the parameters required for sending video data captured from the CCTV cameras connected in a cluster?
The parameters fall into different categories:
1. Network & Transmission Parameters
-IP Address / MAC Address of each camera
-Bandwidth availability (upload/download capacity)
-Transmission Protocol (e.g., RTP, RTSP, HTTP, MQTT)
-Latency and jitter tolerance
-Packet Loss Rate tolerance
-Encoding Protocol (e.g., H.264, H.265)
-Port numbers used for streaming and control

2.Camera Configuration Parameters
-Frame Rate (e.g., 30 fps, 60 fps)
-Resolution (e.g., 1080p, 4K)
-Bitrate (constant or variable)
-Compression method
-Camera ID or Stream Tag for identification in the cluster

3. Security Parameters
-Authentication Credentials (username/password or token)
-Encryption Protocols (e.g., TLS/SSL, SRTP)
-Firewall/NAT configurations (if remote access is involved)

4. Cluster Management Parameters
-Master-Slave Configuration or peer-to-peer protocol
-Load balancing strategy (to avoid overload)
-Time synchronization (e.g., NTP)
-Failover/Recovery setup

### 25(a) Explain with an example the communication complexity /cost of communication of Distributed Computing cluster by equality function.

### 26(a) Function of Management in IoT Systems
### [(CO4)(Analyse/HOCQ)]

In IoT systems, **management functions** play a crucial role in ensuring efficient operation, security, and scalability. The key functions include:

1. **Device Management** - Monitors and maintains IoT devices, including provisioning, authentication, and firmware updates.
2. **Data Management** - Handles data collection, storage, and processing from IoT devices.
3. **Network Management** - Ensures proper communication between devices, cloud services, and edge computing nodes.
4. **Security Management** - Implements authentication, encryption, and anomaly detection to prevent unauthorized access.
5. **Energy Management** - Optimizes power consumption for battery-operated IoT devices.
6. **Fault Management** - Detects and resolves issues in real-time to maintain system reliability.

Efficient IoT management ensures **seamless connectivity, improved performance, and enhanced security** in distributed networks.

---

### 26(b) Role of Subscribers in the Publish-Subscribe Model
### [(CO6)(Remember/LOCQ)]

In the **Publish-Subscribe (Pub-Sub) Model**, **subscribers** play a vital role in receiving relevant data. Their key roles include:

1. **Listening for Messages** - Subscribers register to specific topics of interest and receive only relevant messages.
2. **Decoupling from Publishers** - Subscribers do not communicate directly with publishers; instead, they rely on a broker for message distribution.
3. **Handling Events** - When a publisher sends a message, the broker ensures it reaches all relevant subscribers.
4. **Asynchronous Communication** - Subscribers receive messages asynchronously, making the system scalable and efficient.
5. **Filtering Data** - Subscribers can apply filters to process only necessary messages, reducing bandwidth usage.

Subscribers enable **efficient, scalable, and flexible communication in IoT and distributed systems** by reducing direct dependencies between components.

---

### 26(c) Functions of Queues in Push-Pull Communication Model
### [(CO6)(Apply/IOCQ)]

In the **Push-Pull Communication Model**, **message queues** manage data flow between producers and consumers. Their primary functions include:

1. **Decoupling Producers and Consumers** - Queues act as an intermediary, allowing producers to send messages without waiting for consumers.
2. **Load Balancing** - Messages are stored in a queue until a consumer is ready, preventing system overload.
3. **Message Persistence** - Queues store messages temporarily, ensuring reliable delivery even if a consumer is unavailable.
4. **Prioritization** - Some queue implementations allow prioritizing urgent messages for faster processing.
5. **Scalability** - Multiple consumers can pull messages from the queue, improving system performance under high loads.
6. **Fault Tolerance** - If a consumer fails, messages remain in the queue until successfully processed by another consumer.

This model is widely used in **distributed systems, cloud services, and IoT networks** to improve communication reliability and efficiency.

---

### 27(a) Comparison of REST-Based Communication APIs vs. WebSocket-Based Communication APIs
### [(CO6)(Analyse/HOCQ)]

| Feature                | REST-Based Communication API | WebSocket-Based Communication API |
|------------------------|-----------------------------|-----------------------------------|
| **Communication Type** | Request-Response (Synchronous) | Full-Duplex (Asynchronous) |
| **Connection**         | Stateless, new connection for each request | Persistent, single connection |
| **Data Transfer**      | Each request gets a separate response | Continuous data exchange |
| **Latency**           | Higher due to repeated handshakes | Lower since connection is persistent |
| **Best Use Case**      | Suitable for standard web applications (e.g., RESTful APIs) | Ideal for real-time applications (e.g., chat, gaming, stock updates) |
| **Scalability**       | Scales well with caching and load balancing | Requires persistent connections, may increase resource consumption |

### Key Analysis:
- **REST is best for request-response interactions**, while **WebSockets are optimal for real-time bidirectional communication**.
- **WebSockets reduce latency and bandwidth** but require **more resources** to maintain persistent connections.
- REST is widely used in **web services and IoT APIs**, while WebSockets are better for **real-time IoT applications** like smart home automation and live monitoring.

---

### 27(b) Exclusive Pair Communication Model
### [(CO6)(Remember/LOCQ)]

The **Exclusive Pair Communication Model** is a **one-to-one communication mechanism** where two devices establish a **direct and persistent** connection. 

### Features:
1. **Dedicated Connection** - A stable, continuous link between two devices.
2. **Low Latency** - Ensures fast and reliable data transfer.
3. **Secure Communication** - Encryption and authentication mechanisms enhance security.
4. **Limited Scalability** - Not suitable for systems requiring multiple concurrent connections.
5. **Example Use Cases** - Used in **Bluetooth Pairing, Secure IoT Messaging, and Encrypted Device Communication**.

This model is **best suited for IoT applications where data privacy, reliability, and low latency are critical**.

---

### 27(c) Applications of IoT Enabling Technologies
### [(CO5)(Apply/IOCQ)]

Several technologies enable IoT applications across industries. Some key applications include:

1. **Smart Homes & Cities** - IoT-enabled lighting, energy management, and smart grids.
2. **Healthcare & Wearables** - Remote patient monitoring, smartwatches, and fitness trackers.
3. **Industrial Automation** - Predictive maintenance, robotics, and supply chain tracking.
4. **Agriculture** - Smart irrigation systems, soil monitoring, and automated farming.
5. **Transportation & Logistics** - Connected vehicles, GPS tracking, and fleet management.
6. **Retail & Customer Experience** - Smart shelves, personalized shopping experiences, and automated checkout systems.

These applications leverage IoT technologies like **RFID, Cloud Computing, Edge AI, 5G, and Blockchain** to improve efficiency, automation, and connectivity.

---

### 28(a) Comparison of Infrastructure as a Service (IaaS) vs. Platform as a Service (PaaS)
### [(CO6)(Analyse/HOCQ)]

| Feature                | Infrastructure as a Service (IaaS) | Platform as a Service (PaaS) |
|------------------------|----------------------------------|------------------------------|
| **Definition**        | Provides virtualized computing resources over the cloud. | Provides a development environment with tools and services. |
| **Control Level**     | Full control over the infrastructure (servers, storage, networking). | Limited control, focuses on application development and deployment. |
| **Management**       | Users manage operating systems, applications, and storage. | Cloud provider manages OS, runtime, and middleware. |
| **Scalability**       | Highly scalable, users can configure resources as needed. | Scales automatically but within the provider’s framework. |
| **Target Users**      | IT administrators, system architects needing custom infrastructure. | Developers focusing on building and deploying applications. |
| **Examples**         | AWS EC2, Google Compute Engine, Microsoft Azure Virtual Machines. | Google App Engine, AWS Elastic Beanstalk, Microsoft Azure App Services. |

### Key Analysis:
- **IaaS is best for users who need full control over their infrastructure**, while **PaaS is ideal for developers who want a ready-to-use development platform**.
- **IaaS requires more configuration**, whereas **PaaS simplifies development with pre-configured tools**.

---

### 28(b) Cloud Computing Services Offered to Users
### [(CO6)(Remember/LOCQ)]

Cloud computing offers various services categorized into the following:

1. **Infrastructure as a Service (IaaS)** - Provides virtual machines, storage, and networking (e.g., AWS EC2, Google Compute Engine).
2. **Platform as a Service (PaaS)** - Offers a development platform with pre-built tools (e.g., AWS Elastic Beanstalk, Heroku).
3. **Software as a Service (SaaS)** - Provides ready-to-use software over the cloud (e.g., Google Drive, Microsoft 365, Dropbox).
4. **Function as a Service (FaaS)** - Supports serverless computing where functions run on-demand (e.g., AWS Lambda, Google Cloud Functions).
5. **Storage as a Service (STaaS)** - Offers cloud-based storage solutions (e.g., Google Drive, OneDrive, Amazon S3).
6. **Database as a Service (DBaaS)** - Provides managed database services (e.g., AWS RDS, Firebase, Google Cloud SQL).

These services help users **reduce infrastructure costs, increase scalability, and enhance accessibility** in cloud environments.

---

### 28(c) Characteristics of Big Data
### [(CO5)(Analyse/IOCQ)]

Big Data is characterized by the **5Vs**:

1. **Volume** - Large amounts of data generated from various sources like sensors, healthcare devices, and IoT systems.
2. **Velocity** - High-speed data generation and processing in real-time or near real-time.
3. **Variety** - Data comes in multiple formats: structured (databases), semi-structured (JSON, XML), and unstructured (videos, images, logs).
4. **Veracity** - Ensures data accuracy and reliability, as sensor data may contain noise or inconsistencies.
5. **Value** - Extracting meaningful insights and patterns to drive decision-making and innovations.

Big Data technologies process and analyze **massive datasets** to improve efficiency in **healthcare, IoT, predictive maintenance, and AI applications**.

---

### 29(a) Four Stages of IoT Architecture System
### [(CO4)(Analyse/HOCQ)]

IoT systems typically follow a **four-stage architecture**, ensuring seamless data collection, processing, and decision-making.

### **1. Perception Layer (Sensing Layer)**
- Comprises **sensors, actuators, and edge devices** that collect real-world data.
- Examples: Temperature sensors, RFID tags, cameras, GPS modules.
- **Function**: Detects physical parameters and transmits data to the next stage.

### **2. Network Layer**
- Transmits data from the perception layer to the processing system.
- Uses **Wi-Fi, Bluetooth, Zigbee, LoRaWAN, Cellular (4G/5G), or Ethernet**.
- **Function**: Ensures secure and reliable communication between IoT devices and cloud servers.

### **3. Processing Layer (Edge or Fog Computing Layer)**
- Handles **local data processing and decision-making** before sending data to the cloud.
- Includes **edge servers, IoT gateways, and microcontrollers (Arduino, Raspberry Pi, ESP32).**
- **Function**: Reduces latency and network congestion by pre-processing data.

### **4. Application Layer**
- Provides a user interface for monitoring and controlling IoT systems.
- Examples: **Mobile apps, web dashboards, voice assistants (Alexa, Google Assistant).**
- **Function**: Translates data into meaningful insights and enables user interaction.

Each stage works together to create an efficient **end-to-end IoT ecosystem**, ensuring real-time monitoring, automation, and decision-making.

---

### 29(b) Suitable System for Low-Cost, Low-Complexity Solutions
### [(CO5)(Apply/IOCQ)]

For **low-cost and low-complexity IoT applications** like **home automation and smart irrigation**, a suitable system should have:

### **1. Microcontroller-Based Architecture**
- **ESP32 / ESP8266 / Arduino Uno** for simple automation tasks.
- **LoRa / Zigbee / Wi-Fi** for low-power wireless communication.

### **2. Lightweight Data Processing**
- **Edge computing** using **Raspberry Pi** or ESP32 to process small datasets.
- **No need for heavy cloud computing**; simple **local storage (SD card) or Firebase** is sufficient.

### **3. Energy-Efficient and Cost-Effective Sensors**
- **Soil moisture sensors** for smart irrigation.
- **Motion and light sensors** for home automation.
- **DHT11/DHT22 temperature and humidity sensors** for climate control.

### **4. Low-Power Communication Protocols**
- **MQTT (Message Queuing Telemetry Transport)** for lightweight IoT messaging.
- **Bluetooth Low Energy (BLE) or LoRaWAN** for minimal power consumption.

### **Example Use Case**: **Smart Irrigation System**
- **ESP32 collects soil moisture data** and controls water flow.
- **LoRa transmits data to a remote monitoring system**.
- **Mobile app displays real-time soil conditions and automation status**.

By utilizing **low-power microcontrollers, edge computing, and efficient sensors**, such solutions offer an optimal balance between **cost, power consumption, and performance**.

---

### 30(a) IoT System for Noise Monitoring
### [(CO5)(Analyse/HOCQ)]

An IoT-based noise monitoring system is designed to detect and analyze noise levels in different environments, such as **urban areas, industries, and residential zones**. 

### **System Components:**
1. **Sound Sensors (Microphones & Decibel Meters)** - Captures real-time noise levels.
2. **Microcontrollers (ESP32/Raspberry Pi/Arduino)** - Processes the sensor data.
3. **Wireless Communication (Wi-Fi, LoRa, or GSM)** - Sends data to cloud storage.
4. **Cloud/Edge Computing (AWS IoT, Google Cloud IoT, ThingSpeak)** - Analyzes noise trends.
5. **Dashboard & Alerts (Mobile App/Web Interface)** - Displays noise levels and triggers notifications when thresholds are exceeded.

### **Working Mechanism:**
- The **sound sensors measure noise intensity** and send data to a microcontroller.
- The processed data is **transmitted to the cloud** for analysis and visualization.
- If noise levels exceed a predefined limit, **alerts are sent to authorities or users**.
- The system can be used for **pollution monitoring, workplace safety, and smart city initiatives**.

---

### 30(b) IoT-Specific Home Automation Systems
### [(CO5,CO6)(Remember/LOCQ)]

Several IoT-based home automation solutions are widely used for smart living. Some of the key systems include:

1. **Smart Lighting (Philips Hue, LIFX)** - IoT-enabled bulbs controlled via mobile apps or voice assistants.
2. **Smart Thermostats (Nest, Ecobee)** - Automatically adjust home temperature for energy efficiency.
3. **Smart Security Cameras (Ring, Arlo, Wyze)** - Motion-detecting cameras for home surveillance.
4. **Smart Door Locks (August, Yale, Schlage Encode)** - IoT-based keyless entry with remote control.
5. **Smart Plugs (TP-Link Kasa, Wemo)** - Allows remote control of appliances via IoT connectivity.
6. **Voice Assistants (Amazon Alexa, Google Assistant, Apple Siri)** - Centralized control for smart home devices.

---

### 30(c) IoT-Based City Automation Applications
### [(CO5,CO6)(Apply/IOCQ)]

1. **Smart Traffic Management** - IoT sensors and AI optimize traffic flow, reducing congestion and improving transportation efficiency.
2. **Smart Waste Management** - IoT-enabled trash bins send fill-level data to optimize waste collection routes, reducing operational costs.

These applications enhance urban infrastructure, leading to **efficient, sustainable, and connected smart cities**.

### 31(a) Comparison: Air Pollution Monitoring vs. Noise Pollution Monitoring in IoT
### [(CO5)(Analyse/HOCQ)]

| Feature                | Air Pollution Monitoring System | Noise Pollution Monitoring System |
|------------------------|--------------------------------|----------------------------------|
| **Purpose**           | Monitors air quality by detecting pollutants like CO₂, NO₂, PM2.5, etc. | Measures noise levels in dB to assess environmental noise pollution. |
| **Key Sensors**       | Gas sensors (MQ-135, CO₂ sensors), PM2.5 sensors | Microphones, decibel meters (MEMS, Electret Condenser Microphones). |
| **Data Processing**   | Analyzes pollutant concentration trends over time. | Detects noise intensity variations and threshold breaches. |
| **Real-time Alerts**  | Sends warnings if pollution exceeds safe levels. | Notifies authorities if noise pollution surpasses limits. |
| **Use Cases**        | Smart cities, industrial emissions control, vehicle pollution tracking. | Urban noise pollution management, workplace safety, smart campuses. |
| **Impact**           | Reduces respiratory diseases and improves public health. | Helps enforce noise regulations and prevents hearing-related issues. |

Both systems contribute to **environmental sustainability** and **public well-being** under the IoT domain.

---

### 31(b) Machine-to-Machine (M2M) Communication
### [(CO6)(Remember/LOCQ)]

M2M (Machine-to-Machine) communication refers to the **direct exchange of data between devices** without human intervention. It enables automation across various industries using IoT networks.

### **Key Characteristics:**
- **Autonomous Data Exchange** - Devices communicate in real time without manual input.
- **Network Connectivity** - Uses Wi-Fi, Zigbee, LoRa, Cellular (4G/5G), or LPWAN.
- **Remote Monitoring & Control** - Sensors and actuators work together to optimize operations.
- **Minimal Human Intervention** - Reduces errors and enhances efficiency.

M2M plays a crucial role in **smart industries, healthcare, agriculture, and transportation** by enabling seamless automation.

---

### 31(c) Two M2M Applications
### [(CO6)(Apply/IOCQ)]

1. **Smart Metering** - Automated electricity, water, and gas meters transmit consumption data to utility providers, improving billing accuracy and resource management.
2. **Fleet Management** - IoT-enabled vehicle tracking systems monitor fuel usage, route optimization, and maintenance schedules for logistics companies.

M2M applications **enhance operational efficiency, reduce costs, and improve decision-making** in various industries.

### 32(a) Difference Between IoT and M2M Features
### [(CO4)(Analyse/HOCQ)]

| Feature               | Internet of Things (IoT)         | Machine-to-Machine (M2M)       |
|-----------------------|--------------------------------|--------------------------------|
| **Definition**       | A network of connected devices that communicate over the internet. | Direct communication between machines without human intervention. |
| **Communication**    | Uses **IP-based networks (Wi-Fi, LPWAN, 5G, etc.)**. | Uses **dedicated point-to-point connections (Cellular, Zigbee, RFID, etc.)**. |
| **Data Handling**    | Processes data on **cloud-based platforms** for analytics and decision-making. | Data is **exchanged locally** between devices with minimal cloud involvement. |
| **Automation**       | Provides **AI-driven automation and predictive analytics**. | Automates repetitive tasks but **lacks AI-based intelligence**. |
| **Scalability**      | Highly **scalable** and supports billions of devices. | Limited scalability, mainly used in industrial and enterprise settings. |
| **Example Applications** | Smart homes, healthcare monitoring, autonomous vehicles. | Industrial automation, fleet tracking, remote asset monitoring. |

IoT extends M2M functionality by integrating cloud computing, data analytics, and AI for enhanced automation.

---

### 32(b) General Architecture of an M2M System
### [(CO4)(Remember/LOCQ)]

M2M systems consist of multiple components that facilitate **machine-to-machine communication**. The architecture generally includes:

### **1. Device Layer (Machines & Sensors)**
- Comprises physical devices like **sensors, actuators, meters, and controllers**.
- Collects data from the environment (e.g., temperature, pressure, GPS location).

### **2. Communication Layer (Network & Protocols)**
- Transfers data between devices using **cellular networks (4G/5G), LPWAN (LoRa), Zigbee, or RFID**.
- Securely transmits information to processing systems.

### **3. Data Management Layer (Middleware & Processing)**
- Edge gateways or servers filter and process raw data before storage.
- Data transmission protocols like **MQTT, CoAP, or HTTP** are used for connectivity.

### **4. Application Layer (User Interfaces & Decision Systems)**
- Data is analyzed and presented on **dashboards, mobile apps, or cloud platforms**.
- Decision-making and control automation take place here.

This architecture allows M2M systems to operate **efficiently, securely, and in real-time** for various industrial applications.

---

### 32(c) Machines in M2M vs. Things in IoT
### [(CO5)(Apply/IOCQ)]

- **Machines in M2M** are typically **pre-programmed devices** like industrial robots, smart meters, and GPS trackers that operate **autonomously in a fixed network**.
- **Things in IoT** include **smart objects** like wearables, smart home devices, and connected cars that interact with **cloud-based AI systems for real-time decision-making**.

M2M focuses on **direct machine interaction**, while IoT enhances **connectivity, intelligence, and analytics** for broader applications.

### 33(a) Comparison: Data Plane vs. Control Plane in SDN
### [(CO4)(Analyse/HOCQ)]

| Feature            | Data Plane                         | Control Plane                     |
|--------------------|----------------------------------|----------------------------------|
| **Definition**     | Handles packet forwarding and processing. | Manages network rules and traffic policies. |
| **Function**       | Directly processes incoming and outgoing packets. | Determines how data should be forwarded. |
| **Location**       | Found in network devices like routers and switches. | Resides in a centralized SDN controller. |
| **Decision Making**| Works based on instructions from the control plane. | Uses algorithms and policies to optimize routing. |
| **Scalability**    | Scales with hardware-based forwarding capabilities. | Scales through cloud-based or distributed controllers. |
| **Example**       | OpenFlow switches executing forwarding rules. | SDN controllers (e.g., OpenDaylight, ONOS) managing routing. |

The **data plane** is responsible for the **actual movement of packets**, while the **control plane** defines how packets should be handled, improving network flexibility and programmability.

---

### 33(b) Importance of SDN
### [(CO4)(Remember/LOCQ)]

SDN (Software-Defined Networking) is important because it **decouples network control from hardware**, providing benefits such as:

1. **Centralized Control** – SDN enables a single controller to manage multiple network devices, reducing complexity.
2. **Improved Scalability** – Dynamic allocation of resources makes networks more adaptable to varying loads.
3. **Flexibility & Programmability** – Administrators can update and modify network policies without changing hardware.
4. **Enhanced Security** – Centralized monitoring and policy enforcement prevent unauthorized access and threats.
5. **Optimized Performance** – SDN allows intelligent traffic routing and load balancing for better efficiency.

SDN plays a crucial role in **cloud computing, data centers, and 5G networks**, making networking more agile and cost-effective.

---

### 33(c) Three Main Components of SDN
### [(CO4)(Apply/IOCQ)]

SDN consists of three key components:

1. **Application Layer**
   - Includes **network applications** that define services such as traffic routing, firewall rules, and security policies.
   - Examples: Load balancers, intrusion detection systems, and monitoring tools.

2. **Control Layer (SDN Controller)**
   - Acts as the **brain of the SDN** by managing network policies and forwarding rules.
   - Communicates with both the application layer (above) and the infrastructure layer (below).
   - Example Controllers: **OpenDaylight, ONOS, Ryu**.

3. **Infrastructure Layer (Data Plane)**
   - Comprises **physical and virtual network devices** such as routers, switches, and access points.
   - These devices forward traffic based on rules set by the SDN controller.
   - Uses **OpenFlow, NETCONF, or other southbound APIs** for communication.

Together, these layers **enable programmable, agile, and efficient networking**, making SDN a transformative technology in modern networking.

### 34(a) Comparison: SDN vs. Traditional Networking
### [(CO4)(Analyse/HOCQ)]

| Feature                 | Traditional Networking                        | Software-Defined Networking (SDN) |
|-------------------------|----------------------------------------------|----------------------------------|
| **Network Control**     | Control plane is embedded in network devices. | Control plane is centralized in an SDN controller. |
| **Flexibility**         | Static configuration with limited adaptability. | Highly programmable and flexible. |
| **Scalability**         | Difficult to scale due to hardware dependence. | Easily scalable with cloud-based management. |
| **Management**         | Managed individually on each device. | Centralized control for easier management. |
| **Traffic Handling**     | Uses traditional routing protocols (OSPF, BGP). | Dynamic flow-based traffic control. |
| **Security**            | Security policies applied per device. | Global security enforcement through the controller. |
| **Implementation Cost** | High due to specialized hardware. | Lower cost using commodity hardware. |

SDN provides a **more agile, cost-effective, and scalable** approach to networking compared to traditional networking.

---

### 34(b) Advantages of SDN
### [(CO4)(Remember/LOCQ)]

SDN offers multiple benefits that enhance network efficiency and management:

1. **Centralized Control** – Administrators can manage the entire network from a single point.
2. **Flexibility** – Dynamic policy updates without modifying physical devices.
3. **Cost-Effective** – Uses general-purpose hardware instead of expensive network devices.
4. **Improved Security** – Centralized monitoring and security enforcement.
5. **Optimized Traffic Management** – Intelligent routing and load balancing improve performance.
6. **Automation** – Reduces manual configuration through network programming.

SDN simplifies **network operations and enhances adaptability** for modern IT environments.

---

### 34(c) SDN Architecture
### [(CO4)(Apply/IOCQ)]

SDN architecture consists of three key layers:

### **1. Application Layer**
- Contains **network applications** that define services like security, routing, and monitoring.
- Examples: Firewalls, Intrusion Detection Systems (IDS), and QoS applications.

### **2. Control Layer (SDN Controller)**
- Acts as the **central intelligence** of the SDN.
- Manages and configures network behavior using southbound APIs (e.g., OpenFlow).
- Example Controllers: **ONOS, OpenDaylight, Ryu**.

### **3. Infrastructure Layer (Data Plane)**
- Comprises physical and virtual networking devices such as **switches, routers, and access points**.
- Forwards data packets based on the SDN controller's instructions.

### **Communication Interfaces**
- **Northbound APIs**: Connect applications to the SDN controller (e.g., REST APIs).
- **Southbound APIs**: Connect controllers to network devices (e.g., OpenFlow, NETCONF).

This layered architecture **enhances network programmability, automation, and efficiency**, making SDN an essential technology for modern networks.

### 35(a) Comparison: SDN vs. Traditional Networking
### [(CO4)(Analyse/HOCQ)]

| Feature                 | Traditional Networking                        | Software-Defined Networking (SDN) |
|-------------------------|----------------------------------------------|----------------------------------|
| **Network Control**     | Embedded in each network device. | Centralized in an SDN controller. |
| **Flexibility**         | Rigid and static configurations. | Highly programmable and flexible. |
| **Scalability**         | Limited due to hardware dependency. | Easily scalable with virtualized controllers. |
| **Management**         | Managed per device. | Centrally managed via controllers. |
| **Traffic Handling**     | Uses traditional routing protocols. | Implements dynamic flow-based forwarding. |
| **Security**            | Implemented device by device. | Centralized security policies. |
| **Cost** | High, requires specialized hardware. | Lower, can use general-purpose hardware. |

SDN **separates the control plane from the data plane**, making networks more programmable and efficient.

---

### 35(b) Advantages of Network Function Virtualization (NFV)
### [(CO5)(Remember/LOCQ)]

Network Function Virtualization (NFV) offers several advantages:

1. **Cost Savings** – Replaces dedicated network hardware with software-based virtual network functions (VNFs), reducing capital and operational expenses.
2. **Flexibility & Scalability** – Services can be scaled up or down dynamically based on demand.
3. **Rapid Deployment** – New network functions can be deployed quickly without hardware changes.
4. **Automation & Orchestration** – Simplifies network management by automating service deployment and scaling.
5. **Improved Network Efficiency** – Optimizes resource allocation by running multiple virtualized functions on the same physical infrastructure.
6. **Reduced Power Consumption** – Uses fewer physical devices, leading to energy savings.

NFV **enhances the agility of network services**, making it ideal for cloud and telecom applications.

---

### 35(c) Working of NFV (with Block Diagram)
### [(CO5)(Apply/IOCQ)]

### **NFV Architecture & Workflow**
NFV is built on three primary components:

1. **Virtualized Network Functions (VNFs)**
   - Software-based implementations of network functions (e.g., firewalls, routers, load balancers).
2. **NFV Infrastructure (NFVI)**
   - Provides the compute, storage, and networking resources needed to run VNFs.
3. **NFV Management and Orchestration (NFV-MANO)**
   - Handles lifecycle management, automation, and orchestration of VNFs.

### **Block Diagram of NFV Architecture**
![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/nfvarch.png?raw=true)

### **How NFV Works:**
1. **A user requests a network function (e.g., firewall, VPN, load balancer).**
2. **NFV Orchestration deploys the requested VNFs on available NFVI resources.**
3. **The VNF runs in a virtualized environment, processing network traffic dynamically.**
4. **Scaling and optimization are handled automatically by NFV-MANO.**

NFV improves network **efficiency, automation, and cost-effectiveness**, making it a key technology in modern telecom and cloud computing.

### 36(a) Properties of Sensors: Range, Sensitivity, and Resolution
### [(CO5)(Analyse/HOCQ)]

Sensors are key components of IoT systems. The most important properties are:

1. **Range**:
   - Defines the minimum and maximum values the sensor can measure.
   - Example: A temperature sensor with a range of **-40°C to 125°C**.

2. **Sensitivity**:
   - The smallest change in input that causes a detectable change in output.
   - Example: A pressure sensor with **1 Pa sensitivity** detects very small pressure variations.

3. **Resolution**:
   - The smallest measurable unit of the sensor output.
   - Example: A digital thermometer with **0.1°C resolution** can distinguish changes as small as 0.1°C.

Higher **sensitivity and resolution** improve precision, while an appropriate **range** ensures applicability for various conditions.

---

### 36(b) Advantages of Network Function Virtualization (NFV)
### [(CO5)(Remember/LOCQ)]

NFV provides several key benefits:

1. **Reduced Hardware Dependency** – Eliminates reliance on physical network devices by virtualizing functions.
2. **Cost Efficiency** – Decreases capital and operational expenses by using software-based network functions.
3. **Scalability & Flexibility** – Quickly deploy and adjust network services on demand.
4. **Improved Automation** – Simplifies network management with automated provisioning and orchestration.
5. **Faster Service Deployment** – Enables rapid rollout of new network services without hardware changes.
6. **Optimized Resource Utilization** – Allocates compute and networking resources dynamically, improving efficiency.

NFV **modernizes network infrastructure** and enhances service agility for cloud-based and telecom environments.

---

### 36(c) Working of NFV (with Block Diagram)
### [(CO5)(Apply/IOCQ)]

### **NFV Architecture & Workflow**
NFV consists of three main components:

1. **Virtualized Network Functions (VNFs)** – Software-based network services (e.g., firewalls, routers, load balancers).
2. **NFV Infrastructure (NFVI)** – Compute, storage, and networking resources where VNFs are deployed.
3. **NFV Management & Orchestration (NFV-MANO)** – Manages lifecycle, automation, and deployment of VNFs.

### **Block Diagram of NFV Architecture**
![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/nfvarch.png?raw=true)

### **How NFV Works:**
1. **A network function (e.g., firewall) is requested.**
2. **The NFV Orchestrator assigns resources and deploys VNFs.**
3. **The virtual function processes network traffic dynamically.**
4. **Scaling and optimization are managed automatically.**

NFV enhances **network agility, cost-efficiency, and automation**, making it essential for modern telecom and cloud computing networks.

### 37(a) Advantages and Disadvantages of NFV
### [(CO5)(Analyse/HOCQ)]

### **Advantages of Network Function Virtualization (NFV):**
1. **Cost Reduction** – Eliminates the need for dedicated network hardware, reducing CAPEX and OPEX.
2. **Scalability** – Virtualized functions can be dynamically scaled up or down as needed.
3. **Flexibility** – Enables rapid deployment of new network services without hardware changes.
4. **Automation & Orchestration** – Simplifies network management through automated provisioning.
5. **Energy Efficiency** – Reduces power consumption by running multiple VNFs on shared infrastructure.
6. **Improved Service Agility** – Faster rollout of new services and network functions.

### **Disadvantages of NFV:**
1. **Performance Overhead** – Virtualized functions may have slightly lower performance compared to hardware-based solutions.
2. **Security Concerns** – NFV introduces additional attack vectors due to software vulnerabilities.
3. **Complex Management** – Requires efficient orchestration and monitoring to ensure smooth operation.
4. **Integration Challenges** – Legacy network components may not be fully compatible with NFV-based systems.
5. **Reliability Risks** – A failure in the NFV infrastructure can impact multiple virtualized network functions at once.

---

### 37(b) Steps of IoT Methodology
### [(CO3)(Remember/LOCQ)]

IoT systems follow a structured methodology for data collection, processing, and action. The key steps are:

1. **Perception Layer (Sensing & Data Collection):**
   - Sensors and devices collect real-world data (e.g., temperature, motion, humidity).
2. **Network Layer (Data Transmission):**
   - Transfers collected data to cloud or edge servers via WiFi, Bluetooth, LPWAN, or cellular networks.
3. **Edge Computing (Pre-Processing & Filtering):**
   - Processes raw data at the network edge to reduce latency before sending it to the cloud.
4. **Cloud Processing & Storage:**
   - The cloud stores and analyzes the data using AI, ML, or big data analytics.
5. **Application Layer (User Interaction & Action):**
   - The processed data is used to trigger actions (e.g., sending alerts, controlling actuators, displaying insights).
6. **Security & Management:**
   - Ensures data encryption, authentication, and real-time monitoring to prevent cyber threats.

IoT methodology **enables seamless connectivity, real-time processing, and intelligent decision-making.**

---

### 37(c) NFV Architecture
### [(CO4)(Apply/IOCQ)]

### **NFV Architecture Overview**
The NFV architecture consists of three major components:

1. **Virtualized Network Functions (VNFs):**
   - Software-based implementations of network functions (e.g., routers, firewalls, load balancers).
2. **NFV Infrastructure (NFVI):**
   - Provides the compute, storage, and network resources needed to run VNFs.
3. **NFV Management and Orchestration (NFV-MANO):**
   - Responsible for the lifecycle management, automation, and orchestration of VNFs.

### **NFV Architecture Block Diagram**
![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/nfvarch.png?raw=true)

### **How NFV Works:**
1. **Network operators request a virtual function (e.g., firewall, VPN).**
2. **NFV-MANO assigns resources and deploys the required VNFs.**
3. **VNFs process network traffic dynamically based on user demands.**
4. **The system auto-scales, optimizes, and manages resources efficiently.**

NFV **transforms traditional networking by enabling virtualization, automation, and cost reduction.** 
