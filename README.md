# Big Data Semester Preparation

## Group A

### Module 1

### (i)Big data is a term for a collection of data set, so large and complex that it becomes difficult to process using 	

- (a) Cloud
- ✅ **(b) Traditional data processing application**
- (c) Visualization
- (d) Unstructured Data 

### (ii)Challenges of Big data faced by IBM are 	
- (a) 3Es
- ✅ **(b) 3Vs**
- (c) 3Os
- (d) 3Ls
### (iii)Challenges of Big Data include 	
- (a) Captute, Curation 
- (b) Storage ,Search , Sharing 
- (c) Transfer ,analysis and Visualization 
- ✅ **(d) All the above** 

### (iv)Earthscope designed to track North America’s geologic evolution reports as 
- ✅ **(a) Repository** 
- (b) Source 
- (c) Process unit 
- (d) None of the Above 
### (v)Real Time fast data is generated from 	
- (a) Social media Networks 
- (b) Scientific Instruments and mobile Devices 
- (c) Sensor technology and Networks 
- ✅ **(d) All the above** 

### (vi)Example of Semi structured data is 	
- (a) Legacy Data 
- (b) Web 
- ✅ **(c) XML**
- (d) None of the Above 
### (vii)Valence in Big data corresponds to 	
- (a) Dimension by data 
- ✅ **(b) Connectedness**
- (c) Speed 
- (d) Complexity 
### (viii)	Data can be harnessed by 
- (a) OLTP and OLAP
- (b) OLAP and RTAP
- ✅ **(c) Both (a) and (b)** 
- (d) None of the Above 

### Module 2
### (i)Apache Hadoop 	
- ✅ **(a) Open source software framework for Big Data** 
- (b) Storage system of Hadoop for Big Data 
- (c) Programming Model for Big Data 
- (d) Map Reduce Summarizer for Big data 
### (ii)Giraph is 	
- (a) Specialized model for Reduce function 
- ✅ **(b) Specialized model for graph processing** 
- (c) Higher level interactivity using storm 
- (d) None of the above 
### (iii)HDFS is a 	
- (a) Application 
- (b) Map Reduce 
- ✅ **(c) Distributed file system that runs on clusters.**
- (d) Non map reduce application 
### (iv) Streaming Applications are done using 	
- (a) Star, Spark 
- (b) Spark, Flink and HDFS
- ✅ **(c) Both (a) and (b)**
- (d) None of the above 
### (v) Pig and Hive are tools 	
- (a) That simplify programming of Map reduce 
- (b) Deals with scripts and queries 
- ✅ **(c) Both (a) and (b)** 
- (d) None of the above 
### (vi) Major Phases of MapReduce are  	
- (a)Mapping ,Reducing,  
- (b) Shuffling ,Sorting, Combining 
- ✅ **(c)Mapping , Shuffling , Sorting , Reducing**  
- (d) Both (a) and (b)
### (vii)Map Reduce Engine 	
- (a) Has cluster and slaves 
- ✅ **(b) Consists Job Tracker and task Tracker** 
- (c) Keeps the balance 
- (d) 
### (viii) MapReduce application end users can run on top of 	
- (a) YARN without disrupting any processes
- ✅ **(b) YARN , NextGen YARN**
- (c) Client 
- (d) Server 

### Module 3
### (i) Characteristics of IoT are:
- (a) Dynamic and Self Adapting  
- (b) Interoperable communication protocols  
- ✅ **(c) Both (a) and (b)**  
- (d) None of the above  

### (ii) Each IoT device has a Unique Id:
- ✅ **(a) Unique Identifier**  
- (b) Internet Protocol Address  
- (c) Personal Computer  
- (d) Smart Phone  

### (iii) Applications of IoT are:
- (a) Music  
- (b) Home lighting  
- (c) Analytics  
- ✅ **(d) All the above**  

### (iv) IoT device can:
- ✅ **(a) Exchange data with other connected devices and applications**  
- (b) Cannot collect data from other devices  
- (c) Both (a) and (b)  
- (d) None of the above  

### (v) HDMI is:
- (a) High definition multiplier interface  
- ✅ **(b) High definition multimedia interface**  
- (c) High difference media interface  
- (d) None of the above  

### (vi) IEEE 802.11 is for:
- (a) Ethernet  
- (b) WPAN  
- ✅ **(c) WiFi**  
- (d) Mobile Communication  

### (vii) IPv4: Internet Protocol version 4 identifies:
- ✅ **(a) Identifies the devices in the network using hierarchical addressing scheme, total 64 addresses.**   
- (b) 128-bit address scheme  
- (c) Data transfer 250kb/s  
- (d) None of the above 

### (viii) CoAP, MQTT, XMPP, DDS are:
- ✅ **(a) Application Layer Protocol**  
- (b) Presentation Layer Protocol  
- (c) Session Layer Protocol  
- (d) Data Link Layer Protocol  

### Module 4
### (i)It uses HiveQl for data structuring and for writing complicated MapReduce in HDFS	
- (a) Storm
- (b) HDFS
- ✅ **(c) Hive** 
- (d) Pig 
### (ii)	Advantages of Hadoop are 
- (a) Ability to store large amount of data 
- (b)  high flexibility 
- ✅ **(c) Both (a) and (b)** 
- (d) None of the above 
### (iii)	HBase support 
- ✅ **(a) random read and writes**
- (b) write once read many times 
- (c) Both (a) and (b) 
- (d) All the above 
### (iv)	Zookeeper provides services as 
- (a) Maintaining and configuration of information 
- (b) Naming and provide distributed synchronization 
- ✅ **(c) Both (a) and (b)**
- (d) None of the above.
### (v)Web Socket API uses 	
- ✅ **(a) Client Server** 
- (b) Request response model 
- (c) Exclusive Pair model
- (d) None of the above 
### (vi) Paas is 	
- (a) Paradigm as a Service
- (b) Internet Provider as a Service 
- ✅ **(c) Platform as a Service**
- (d) None of the above 
### (vii)	Full duplex communication between client and server takes place in 
- (a) Stateless/ Stateful
- (b) Request Response 
- (c) TCP connections 
- ✅ **(d) Web Sockets**
### (viii)	Coordinator nodes collect data from end nodes and send to the 
- (a) Client
- ✅ **(b) Server** 
- (c) Cloud
- (d) None of the above 

---
### Fill in the blanks

1. Big data refers to a massive amount of data that keeps on growing exponentially with time.
2. Open source framework like **Hadoop** was essential for the growth of Big Data.	
3. **Unstructured data** refers to data that lacks any specific form or structure.
4. **Predictive analytics** is a statistical method that utilizes algorithms and machine learning to identify trends in data and predict future behaviour.	
5. Matrix and algebra operations can be performed using **Map Reduce**.
6. Matrix Multiplication can be done by **cascade** of two Map Reduce operations.
7. **Reduce** phase combines all values associated with an identical key.
8. Once shuffling is done, the output is sent to the **sorting** phase where all the (key, value) pairs are sorted automatically.
9. IoT function Block provides the system capabilities for **sensing**, **actuation monitoring**, and **control functions**.
10. Authentication, authorization, and context integrity come under **Security** systems.
11. Request-Respond, Publish–Subscribe are IoT **communication** models.
12. IoT is enabled by several technologies including **Wireless Sensor Networks (WSN)**, **Cloud Computing**, and **Big Data Analysis**.
13. A RESTful web service is a web API implemented using **HTTP** and **REST** principles.
14. IoT applications provide an interface that users can use to control and monitor various aspects of **IoT Systems**.
15. REST services operate over **HTTP**, and each request is independent of each other.
16. **Apache Spark** is an open-source big data processing framework built around speed, ease of use, and sophisticated analysis, developed at UC Berkeley’s AMP Lab.

## Group B

### 2(a) What do you mean by Big data how is it different from the traditional data ? 

**Big Data** refers to extremely large and complex datasets that cannot be handled by traditional data processing tools. It is typically defined by the **3Vs**:

- **Volume** – Huge amount of data (terabytes to zettabytes)
- **Velocity** – High speed of data generation and processing
- **Variety** – Multiple types of data (structured, semi-structured, unstructured)

### Difference Between Big Data and Traditional Data

| Feature             | Traditional Data                         | Big Data                                         |
|--------------------|-------------------------------------------|--------------------------------------------------|
| **Volume**         | MBs to GBs                                | TBs to PBs and beyond                            |
| **Variety**        | Mostly structured                         | Structured, semi-structured, unstructured        |
| **Velocity**       | Batch processing                          | Real-time or near real-time processing           |
| **Tools**          | SQL, Excel                                | Hadoop, Spark, NoSQL, Kafka                      |
| **Storage**        | Centralized databases                     | Distributed file systems (HDFS, S3, etc.)        |
| **Scalability**    | Vertical (limited)                        | Horizontal (distributed clusters)                |

---

### 2(b) Write all the big data Frameworks. 

Here are popular frameworks used in Big Data:

- **Apache Hadoop**
- **Apache Spark**
- **Apache Flink**
- **Apache Storm**
- **Apache Kafka**
- **Apache Hive**
- **Apache Pig**
- **HBase**

---

### 2(c) Describe the tools that are required to handle big data.

Big Data workflows rely on multiple categories of tools:

#### Storage Tools
- **HDFS (Hadoop Distributed File System)**
- **Amazon S3**
- **Google Cloud Storage**
- **MongoDB / Cassandra**

#### Processing & Computation Tools
- **Apache Spark**
- **Apache Flink**
- **Apache Storm**
- **MapReduce**

#### Querying and Analysis Tools
- **Apache Hive**
- **Apache Impala**
- **Apache Drill**
- **Presto**

#### Visualization & BI Tools
- **Tableau**
- **Power BI**
- **Apache Superset**
- **Grafana**

#### Machine Learning & Analytics
- **Apache Mahout**
- **MLlib (Spark)**
- **TensorFlow / PyTorch**

#### Data Ingestion & Orchestration
- **Apache Kafka**
- **Apache NiFi**
- **Apache Airflow**
- **Apache Sqoop**

---

### 3(a) Compare structured data from unstructured and semi structured data 

| Criteria                | Structured Data                           | Semi-Structured Data                       | Unstructured Data                          |
|------------------------|--------------------------------------------|--------------------------------------------|---------------------------------------------|
| **Definition**          | Clearly defined format, stored in tables  | Partial organization with tags/markers     | No predefined format or structure           |
| **Storage**             | RDBMS (e.g., MySQL, PostgreSQL)            | XML, JSON, NoSQL databases                  | Text files, images, videos, PDFs            |
| **Schema**              | Fixed schema                               | Flexible schema                             | No schema                                   |
| **Query Language**      | SQL                                         | XPath, XQuery, custom parsers               | NLP/AI-based parsing or manual methods      |
| **Examples**            | Employee database, bank records            | XML-based config, JSON APIs                 | Social media posts, photos, audio files     |
| **Ease of Analysis**    | High (well-structured)                     | Medium (requires parsing)                   | Low (needs complex processing)              |

---

### 3(b) Explain the software stack of Big data.

The Big Data stack is a layered architecture consisting of the following components:

#### 1. **Data Sources**
- Sensors, Web Logs, Social Media, Mobile Apps, IoT devices

#### 2. **Data Ingestion**
- **Apache Kafka**, **Apache Flume**, **Apache Sqoop**, **Apache NiFi**

#### 3. **Data Storage**
- **HDFS (Hadoop Distributed File System)**
- **Amazon S3**, **Google Cloud Storage**
- **NoSQL Databases**: HBase, Cassandra, MongoDB

#### 4. **Data Processing**
- **Batch Processing**: Apache Hadoop (MapReduce), Apache Spark
- **Stream Processing**: Apache Flink, Apache Storm, Kafka Streams

#### 5. **Data Querying & Analysis**
- **SQL-on-Hadoop**: Apache Hive, Apache Impala, Presto
- **Interactive Tools**: Apache Drill

#### 6. **Data Orchestration**
- **Workflow Management**: Apache Airflow, Apache Oozie

#### 7. **Visualization & BI**
- **Tableau**, **Power BI**, **Apache Superset**, **Grafana**

---

### 3(c)Describe the physical organization of computer nodes in distributed file systems.

In a distributed file system like HDFS, data is stored across multiple nodes organized into a cluster.

#### Components of a Distributed File System:

- **NameNode (Master)**:
  - Manages metadata and namespace
  - Keeps track of file directory structure and block locations

- **DataNodes (Workers)**:
  - Store actual data blocks
  - Perform read/write operations as instructed by the NameNode

#### Data Distribution:
- Files are split into fixed-size blocks (e.g., 128 MB)
- Each block is replicated (default: 3 copies) across different DataNodes for fault tolerance

#### Node Organization:
- Nodes are often grouped in **racks**
- **Rack awareness** policy ensures data is replicated across racks to improve fault tolerance and reduce data loss risk
- Communication between nodes occurs over a high-speed network (Gigabit or 10G Ethernet)

#### Fault Tolerance:
- If a DataNode fails, the system uses replicated blocks from other nodes
- NameNode monitors DataNode health using heartbeat signals

#### Visual Representation
![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/Namenode-and-Datanode.png?raw=true)
---

### 4(a) Compare the role of different Distributed File Systems .

| Distributed File System | Role & Features                                                                 | Use Cases                                   |
|-------------------------|----------------------------------------------------------------------------------|---------------------------------------------|
| **HDFS (Hadoop DFS)**   | Fault-tolerant, block-based storage system designed for large-scale batch data   | Hadoop ecosystem, Big Data analytics        |
| **Google File System (GFS)** | Proprietary system by Google, fault-tolerant, optimized for Google's workload | Web indexing, Search engine backend         |
| **Amazon S3**           | Object-based storage with scalability, durability, and pay-as-you-go pricing     | Cloud backups, Web hosting, Data lakes      |
| **Azure Data Lake Storage** | Optimized for analytics workloads with hierarchical namespace                 | Azure Big Data solutions                    |

---

### 4(b) How Are Large File System Organizations Dealt?

Large file systems face challenges like managing billions of files, ensuring high availability, and handling large file sizes. The following strategies are used:

#### Strategies to Handle Large File Systems:

- **Block-Based Storage**: Files are split into fixed-size blocks (e.g., 64MB or 128MB in HDFS) for distributed storage.
- **Replication**: Blocks are replicated across nodes to ensure fault tolerance and high availability.
- **Metadata Management**: A centralized or distributed metadata service (e.g., NameNode in HDFS) keeps track of file locations and structure.
- **Namespace Scaling**: Use of partitioned namespaces or hierarchical file structures to manage large directories efficiently.
- **Compression and Encoding**: Reduces storage footprint and increases I/O efficiency.
- **Striping**: Data is striped across disks or nodes for parallel access and faster read/write operations.
- **Caching and Buffering**: Frequently accessed files are cached to reduce latency.

---

### 4(c) Explain the Map Reduce Paradigm. How can it be applied?

**MapReduce** is a programming model for processing and generating large datasets with a parallel, distributed algorithm on a cluster.

#### Two Main Phases:

1. **Map Phase**:
   - Input data is split into `<key, value>` pairs.
   - Each pair is processed independently to produce intermediate `<key, value>` pairs.
   - Example: Counting words → input: `"cat cat dog"`, output: `("cat", 1), ("cat", 1), ("dog", 1)`

2. **Reduce Phase**:
   - Intermediate pairs are grouped by key and reduced to a single output per key.
   - Example: `("cat", [1,1])` → `("cat", 2)`

#### How It Works:

- Input files are divided into chunks and processed in parallel by **mappers**.
- Intermediate results are **shuffled and sorted**.
- Results are then processed by **reducers** to generate final output.

#### Applications of MapReduce:

- **Word Count**: Counting word occurrences in large documents
- **Log Analysis**: Processing server logs at scale
- **Inverted Indexing**: Search engine indexing
- **Data Mining**: Large-scale pattern detection
- **ETL Pipelines**: Data transformation and loading tasks

---

### 5(a) Comparison: Role of HDFS, YARN, and HBase

| Component | Full Form                          | Role in Hadoop Ecosystem                                                                 |
|-----------|------------------------------------|-------------------------------------------------------------------------------------------|
| **HDFS**  | Hadoop Distributed File System     | A distributed storage system that stores large files across multiple machines. It ensures fault tolerance, scalability, and high throughput for large data sets. |
| **YARN**  | Yet Another Resource Negotiator    | Acts as the **resource manager** of Hadoop. It allocates system resources and schedules jobs among applications. Enables multi-tenancy and scalability. |
| **HBase** | Hadoop Database                    | A **NoSQL, column-oriented** distributed database that runs on top of HDFS. It supports random, real-time read/write access to large datasets. |

---

### 5(b) Functions of Combiners

**Combiners** are mini-reducers used in the **MapReduce** framework to optimize performance by minimizing data transfer between the Map and Reduce phases.

#### Key Functions:
- Operate **after the Map phase** and **before the Reduce phase**.
- Perform **local aggregation** of intermediate outputs.
- Reduce the volume of data that is shuffled and sorted across the network.
- Useful for operations like **sum**, **count**, **min**, **max**, etc.

> Example: In a word count task, a combiner can sum word counts locally on each mapper before sending to the reducer.

---

### 5(c) Comparison Parameters: SQL vs NoSQL

| Parameter             | SQL (Relational DBs)                  | NoSQL (Non-Relational DBs)                    |
|-----------------------|----------------------------------------|-----------------------------------------------|
| **Data Model**        | Tabular (rows and columns)             | Key-Value, Document, Column-Family, Graph     |
| **Schema**            | Fixed, predefined schema               | Dynamic, flexible schema                      |
| **Scalability**       | Vertical (scale-up)                    | Horizontal (scale-out)                        |
| **Transactions**      | ACID compliant                         | BASE properties (eventual consistency)        |
| **Examples**          | MySQL, PostgreSQL, Oracle              | MongoDB, Cassandra, Redis           |
| **Best For**          | Structured data, complex joins         | Unstructured/semi-structured data, large scale |
| **Query Language**    | SQL (Structured Query Language)        | Varies: JSON-based queries, APIs              |
| **Data Integrity**    | High                                    | Depends on DB type and configuration          |

---

### 6(a) How Do Combiners Work? Explain with a diagram.

**Combiners** are optional components in the **MapReduce** framework that perform **local aggregation** of intermediate map outputs before they are sent across the network to the reducers.

![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/mapreduce.png?raw=true)

---

### 6(b) Advantages of Combiners
- **Minimize Data Transfer** : Reduces the amount of data shuffled between Mappers and Reducers.
- **Improve Performance** : Speeds up MapReduce jobs by decreasing network congestion.
- **Local Optimization** : Performs computation locally before sending to the reducer.
- **Effective for Aggregations** : Especially useful in word count, sum, average, and min/max operations.
- **Better Resource Utilization** : Reduces I/O and network bandwidth usage.

---

### 6(c) Components of MapReduce Architecture
Component	Description
1. **Input Data** :	Raw data split into blocks (input splits) and processed in parallel
2. **Mapper** :	Processes each input split to generate intermediate key-value pairs
3. **Combiner	(Optional)**: Aggregates mapper output locally before shuffle
4. **Partitioner** :	Assigns intermediate keys to reducers based on a partitioning logic
5. **Shuffle & Sort** :	Transfers intermediate data to reducers and sorts by key
6. **Reducer** :	Aggregates and processes grouped intermediate data to generate final output
7. **Output Format** :	Writes the final output to HDFS or another file system

### 7(a) Briefly explain the method to find relative frequency and cumulative relative frequency using numerical example .

#### Definitions:
- **Relative Frequency**: Frequency of a value divided by total number of values.
- **Cumulative Relative Frequency**: Running total of relative frequencies up to a certain point.

### Example:

| Value | Frequency |
|-------|-----------|
| 10    | 2         |
| 20    | 3         |
| 30    | 5         |

**Step 1: Total Frequency** = 2 + 3 + 5 = 10

**Step 2: Calculate Relative Frequency**

| Value | Frequency | Relative Frequency (f/N) |
|-------|-----------|---------------------------|
| 10    | 2         | 2/10 = 0.2                 |
| 20    | 3         | 3/10 = 0.3                 |
| 30    | 5         | 5/10 = 0.5                 |

**Step 3: Cumulative Relative Frequency**

| Value | Cumulative Relative Frequency |
|-------|-------------------------------|
| 10    | 0.2                           |
| 20    | 0.2 + 0.3 = 0.5               |
| 30    | 0.5 + 0.5 = 1.0               |

---

### 7(b) What is Relative Frequency?

**Relative Frequency** is the ratio of the number of times a specific value occurs to the total number of observations.

#### Formula:
**Relative Frequency = Frequency of a value / Total number of values**

#### Example:
If a value appears **4 times** in a dataset of **20 values**, its relative frequency is:

**Relative Frequency = 4 / 20 = 0.2**

---

### 7(c) Compute the Mean using the Mapper class and Reducer Class .
```python
# Mapper Class
class Mapper:
    def map(self, values):
        for value in values:
            yield ("sum", value)
            yield ("count", 1)

# Reducer Class
class Reducer:
    def reduce(self, mapped_data):
        result = {"sum": 0, "count": 0}
        for key, value in mapped_data:
            result[key] += value

        mean = result["sum"] / result["count"] if result["count"] != 0 else 0
        return mean

# Example usage
if __name__ == "__main__":
    data = [10, 20, 30, 40, 50]
    mapper = Mapper()
    reducer = Reducer()

    # Simulate map phase
    mapped_output = list(mapper.map(data))

    # Group by keys
    shuffled = {}
    for key, value in mapped_output:
        shuffled.setdefault(key, []).append(value)

    # Flatten grouped data for reduce phase
    flattened = [(k, v) for k, values in shuffled.items() for v in values]

    # Reduce phase
    mean_result = reducer.reduce(flattened)
    print("Mean:", mean_result)
```
---

### 8(a) Compare the role of Pair and Strips . Which framework are they associated?

#### Associated Framework:
Both **Pair** and **Stripes** approaches are used in **Hadoop MapReduce** for **co-occurrence matrix computation**, especially in Natural Language Processing (NLP) and text mining tasks.

---

#### Comparison Table

| Aspect                  | **Pair Approach**                                      | **Stripes Approach**                                   |
|-------------------------|--------------------------------------------------------|--------------------------------------------------------|
| **Key Format**          | Uses a **composite key** like `(word1, word2)`         | Uses a **single key** like `word1`, value is a hashmap |
| **Emitted Data Size**   | More key-value pairs (e.g., one for every pair)         | Fewer pairs; emits one map (dictionary) per word       |
| **Efficiency**          | Higher shuffle and sort cost due to more key-value pairs | More efficient for frequent words (lower network I/O) |
| **Memory Usage**        | Low — no need to store hashmaps                         | Higher — maintains an in-memory hashmap                |
| **Reducer Complexity**  | Simple, since input keys are already granular           | Complex — needs to merge dictionaries                  |
| **Example Output**      | `(word1, word2) → 1`                                    | `word1 → {word2: count, word3: count}`                 |
| **Use Case**            | Suitable when memory is constrained                     | Suitable when memory is sufficient for aggregation     |

---

### 8(b) Explain the function of Combiners. 

**Combiners** are mini-reducers used in the **MapReduce** framework to optimize performance by minimizing data transfer between the Map and Reduce phases.

#### Key Functions:
- Operate **after the Map phase** and **before the Reduce phase**.
- Perform **local aggregation** of intermediate outputs.
- Reduce the volume of data that is shuffled and sorted across the network.
- Useful for operations like **sum**, **count**, **min**, **max**, etc.

> Example: In a word count task, a combiner can sum word counts locally on each mapper before sending to the reducer.

---

### 8(c) Describe briefly the components of Map Reduce Architecture.

Component	Description
1. **Input Data** :	Raw data split into blocks (input splits) and processed in parallel
2. **Mapper** :	Processes each input split to generate intermediate key-value pairs
3. **Combiner	(Optional)**: Aggregates mapper output locally before shuffle
4. **Partitioner** :	Assigns intermediate keys to reducers based on a partitioning logic
5. **Shuffle & Sort** :	Transfers intermediate data to reducers and sorts by key
6. **Reducer** :	Aggregates and processes grouped intermediate data to generate final output
7. **Output Format** :	Writes the final output to HDFS or another file system

--- 

### 9(a) Compare the role of Mapper and Reducer function .

| Feature              | **Mapper**                                                    | **Reducer**                                                  |
|----------------------|---------------------------------------------------------------|--------------------------------------------------------------|
| **Position in Flow** | Executes first, processes input data                          | Executes after Shuffle & Sort, processes intermediate data   |
| **Input**            | Raw input split (e.g., lines of text, key-value records)      | Grouped key and list of associated values                    |
| **Output**           | Intermediate key-value pairs                                  | Final output key-value pairs                                 |
| **Purpose**          | Transforms and filters raw data                               | Aggregates and summarizes intermediate data                  |
| **Parallelism**      | Highly parallel (1 per input split)                           | Limited by number of output keys                             |
| **Examples**         | Tokenizing, Filtering, Preprocessing                          | Counting, Summing, Reducing lists of values                  |
| **Stateless/Stateful**| Typically stateless                                          | Can perform stateful aggregation                             |

---

### 9(b) What are the tasks for which Mapper and Reducer classes can be written? Give an Example.

#### Typical Use Cases

| Task                          | Mapper Role                                      | Reducer Role                                       |
|-------------------------------|--------------------------------------------------|----------------------------------------------------|
| **Word Count**                | Emits each word with count `1`                  | Sums counts for each word                          |
| **Sorting**                   | Emits sortable key-value pairs                   | Aggregates or formats sorted keys                  |
| **Inverted Indexing**         | Emits (word, documentID)                         | Aggregates document IDs per word                   |
| **Log Analysis**              | Emits (IP, 1) or (StatusCode, 1)                 | Counts occurrences of IPs or status codes          |
| **Join Operations**           | Emits key and source-tagged value from both sets | Merges/join records based on key                   |

#### Word Count Example:

```python
# Mapper emits:
("apple", 1)
("banana", 1)
("apple", 1)

# Reducer receives:
("apple", [1, 1]) → ("apple", 2)
("banana", [1])   → ("banana", 1)
```

### 9(c) Describe briefly the functions Distributed file systems.

A Distributed File System (DFS) like HDFS or GFS allows storage and access to data across multiple machines as if it were on a single file system.

#### Key Functions:

- **Data Distribution** : Splits large files into blocks and distributes across nodes.
- **Fault Tolerance** : Replicates data blocks across multiple nodes to avoid data loss.
- **Scalability** : Easily scaled horizontally by adding more nodes.
- **High Availability** : Ensures data is accessible even if some nodes fail.
- **Parallel Access** : Multiple clients can read/write data concurrently.
- **Metadata Management** : Maintains a namespace and file-to-block mapping via a central master (e.g., NameNode in HDFS).
- **Security and Access Control** : Manages user permissions, authentication, and secure access.

---

### 10(a) Difference Between Traditional and Distributed File Systems

| Feature                          | **Traditional File System**                                     | **Distributed File System (DFS)**                             |
|----------------------------------|------------------------------------------------------------------|----------------------------------------------------------------|
| **Storage Location**             | Stores data on a **single physical machine**                    | Stores data **across multiple machines** (nodes)               |
| **Scalability**                  | Limited to the capacity of one system                           | Scales horizontally by adding nodes                           |
| **Fault Tolerance**              | If the machine fails, data can be lost                          | Uses **replication** to ensure fault tolerance                 |
| **Performance**                  | Dependent on a single system’s I/O                              | Parallel data access improves performance                     |
| **Data Sharing**                 | Hard to share across systems                                    | Provides transparent access from multiple systems             |
| **Examples**                     | NTFS, FAT32, ext3                                               | HDFS (Hadoop), GFS (Google), Ceph                              |
| **Use Case**                     | Personal computers, small-scale servers                         | Big Data, Cloud Storage, Distributed Computing                 |
| **Data Access**                  | Accessed locally by the same system                             | Can be accessed by multiple systems over a network             |
| **Management**                   | Simple file hierarchy and metadata management                   | More complex with block-level metadata and node management     |

---

### 10(b) Write the basic mapper and reducer function for calculating mean.   

```python
# Mapper Class
class Mapper:
    def map(self, values):
        for value in values:
            yield ("sum", value)
            yield ("count", 1)

# Reducer Class
class Reducer:
    def reduce(self, mapped_data):
        result = {"sum": 0, "count": 0}
        for key, value in mapped_data:
            result[key] += value

        mean = result["sum"] / result["count"] if result["count"] != 0 else 0
        return mean

# Example usage
if __name__ == "__main__":
    data = [10, 20, 30, 40, 50]
    mapper = Mapper()
    reducer = Reducer()

    # Simulate map phase
    mapped_output = list(mapper.map(data))

    # Group by keys
    shuffled = {}
    for key, value in mapped_output:
        shuffled.setdefault(key, []).append(value)

    # Flatten grouped data for reduce phase
    flattened = [(k, v) for k, values in shuffled.items() for v in values]

    # Reduce phase
    mean_result = reducer.reduce(flattened)
    print("Mean:", mean_result)
```
---

### 10(d) Describe briefly the Map-Reduce architecture. Give example of Framework following it.

Component	Description
1. **Input Data** :	Raw data split into blocks (input splits) and processed in parallel
2. **Mapper** :	Processes each input split to generate intermediate key-value pairs
3. **Combiner	(Optional)**: Aggregates mapper output locally before shuffle
4. **Partitioner** :	Assigns intermediate keys to reducers based on a partitioning logic
5. **Shuffle & Sort** :	Transfers intermediate data to reducers and sorts by key
6. **Reducer** :	Aggregates and processes grouped intermediate data to generate final output
7. **Output Format** :	Writes the final output to HDFS or another file system

| Framework       | Description                                                                 |
|-----------------|-----------------------------------------------------------------------------|
| **Apache Hadoop** | The most popular open-source implementation of MapReduce. Uses HDFS + YARN. |
| **Apache Hive**   | SQL-like querying on large datasets, translates queries into MapReduce jobs. |
| **Apache Pig**    | High-level platform for data transformation; internally uses MapReduce.    |

---

### 11(a) Compare the traditional file system from distributed file systems.

| Feature                          | **Traditional File System**                                     | **Distributed File System (DFS)**                             |
|----------------------------------|------------------------------------------------------------------|----------------------------------------------------------------|
| **Storage Location**             | Stores data on a **single physical machine**                    | Stores data **across multiple machines** (nodes)               |
| **Scalability**                  | Limited to the capacity of one system                           | Scales horizontally by adding nodes                           |
| **Fault Tolerance**              | If the machine fails, data can be lost                          | Uses **replication** to ensure fault tolerance                 |
| **Performance**                  | Dependent on a single system’s I/O                              | Parallel data access improves performance                     |
| **Data Sharing**                 | Hard to share across systems                                    | Provides transparent access from multiple systems             |
| **Examples**                     | NTFS, FAT32, ext3                                               | HDFS (Hadoop), GFS (Google), Ceph                              |
| **Use Case**                     | Personal computers, small-scale servers                         | Big Data, Cloud Storage, Distributed Computing                 |
| **Data Access**                  | Accessed locally by the same system                             | Can be accessed by multiple systems over a network             |
| **Management**                   | Simple file hierarchy and metadata management                   | More complex with block-level metadata and node management     |

---

### 11(b) What do you understand from the Map Reduce Paradigm?

**MapReduce** is a programming model for processing and generating large datasets with a parallel, distributed algorithm on a cluster.

#### Two Main Phases:

1. **Map Phase**:
   - Input data is split into `<key, value>` pairs.
   - Each pair is processed independently to produce intermediate `<key, value>` pairs.
   - Example: Counting words → input: `"cat cat dog"`, output: `("cat", 1), ("cat", 1), ("dog", 1)`

2. **Reduce Phase**:
   - Intermediate pairs are grouped by key and reduced to a single output per key.
   - Example: `("cat", [1,1])` → `("cat", 2)`

#### How It Works:

- Input files are divided into chunks and processed in parallel by **mappers**.
- Intermediate results are **shuffled and sorted**.
- Results are then processed by **reducers** to generate final output.

--- 

### 11(c) Name the Big data Frameworks and their area of usage.

Big Data workflows rely on multiple categories of tools:

#### Storage Tools
- **HDFS (Hadoop Distributed File System)**
- **Amazon S3**
- **Google Cloud Storage**
- **MongoDB / Cassandra**

#### Processing & Computation Tools
- **Apache Spark**
- **Apache Flink**
- **Apache Storm**
- **MapReduce**

#### Querying and Analysis Tools
- **Apache Hive**
- **Apache Impala**
- **Apache Drill**
- **Presto**

#### Visualization & BI Tools
- **Tableau**
- **Power BI**
- **Apache Superset**
- **Grafana**

#### Machine Learning & Analytics
- **Apache Mahout**
- **MLlib (Spark)**
- **TensorFlow / PyTorch**

#### Data Ingestion & Orchestration
- **Apache Kafka**
- **Apache NiFi**
- **Apache Airflow**
- **Apache Sqoop**

---

### 12(a) What is the Importance of Big Data Analysis?

Big Data Analysis plays a critical role in modern data-driven decision-making across industries. Here's why it's important:

- **Improved Decision Making**: Enables organizations to make more informed, real-time decisions.
- **Customer Insights**: Helps in understanding customer behavior and preferences.
- **Cost Efficiency**: Identifies inefficiencies and areas to reduce operational costs.
- **Innovation**: Drives innovation by revealing hidden patterns and trends.
- **Operational Efficiency**: Automates and optimizes business processes.
- **Risk Management**: Detects fraud and security breaches through pattern analysis.

---

### 12(b) Describe Any Distributed File System

#### Hadoop Distributed File System

In a distributed file system like HDFS, data is stored across multiple nodes organized into a cluster.

#### Components of a Distributed File System:

- **NameNode (Master)**:
  - Manages metadata and namespace
  - Keeps track of file directory structure and block locations

- **DataNodes (Workers)**:
  - Store actual data blocks
  - Perform read/write operations as instructed by the NameNode

#### Data Distribution:
- Files are split into fixed-size blocks (e.g., 128 MB)
- Each block is replicated (default: 3 copies) across different DataNodes for fault tolerance

#### Node Organization:
- Nodes are often grouped in **racks**
- **Rack awareness** policy ensures data is replicated across racks to improve fault tolerance and reduce data loss risk
- Communication between nodes occurs over a high-speed network (Gigabit or 10G Ethernet)

#### Fault Tolerance:
- If a DataNode fails, the system uses replicated blocks from other nodes
- NameNode monitors DataNode health using heartbeat signals

#### Visual Representation

![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/Namenode-and-Datanode.png?raw=true)

---

### 12(c) Write the functions of mapper and reducer class.

Below is an example of using Python to simulate MapReduce for computing **word count**:

#### Mapper and Reducer Function

```python
from collections import defaultdict
def mapper(text):
    mapped_output = []
    for line in text.strip().split("\n"):
        for word in line.strip().split():
            mapped_output.append((word.lower(), 1))
    return mapped_output
def reducer(mapped_data):
    reduced_output = defaultdict(int)
    for word, count in mapped_data:
        reduced_output[word] += count
    return dict(reduced_output)
if __name__ == "__main__":
    input_text = "big data is big\nbig data is powerful"
    mapped = mapper(input_text)
    result = reducer(mapped)
    print(result)
```

**Output**
`{'big': 3, 'data': 2, 'is': 2, 'powerful': 1}`

---


## Group D


### 26(a) Function of Management in IoT Systems
### [(CO4)(Analyse/HOCQ)]

In IoT systems, **management functions** play a crucial role in ensuring efficient operation, security, and scalability. The key functions include:

1. **Device Management** - Monitors and maintains IoT devices, including provisioning, authentication, and firmware updates.
2. **Data Management** - Handles data collection, storage, and processing from IoT devices.
3. **Network Management** - Ensures proper communication between devices, cloud services, and edge computing nodes.
4. **Security Management** - Implements authentication, encryption, and anomaly detection to prevent unauthorized access.
5. **Energy Management** - Optimizes power consumption for battery-operated IoT devices.
6. **Fault Management** - Detects and resolves issues in real-time to maintain system reliability.

Efficient IoT management ensures **seamless connectivity, improved performance, and enhanced security** in distributed networks.

---

### 26(b) Role of Subscribers in the Publish-Subscribe Model
### [(CO6)(Remember/LOCQ)]

In the **Publish-Subscribe (Pub-Sub) Model**, **subscribers** play a vital role in receiving relevant data. Their key roles include:

1. **Listening for Messages** - Subscribers register to specific topics of interest and receive only relevant messages.
2. **Decoupling from Publishers** - Subscribers do not communicate directly with publishers; instead, they rely on a broker for message distribution.
3. **Handling Events** - When a publisher sends a message, the broker ensures it reaches all relevant subscribers.
4. **Asynchronous Communication** - Subscribers receive messages asynchronously, making the system scalable and efficient.
5. **Filtering Data** - Subscribers can apply filters to process only necessary messages, reducing bandwidth usage.

Subscribers enable **efficient, scalable, and flexible communication in IoT and distributed systems** by reducing direct dependencies between components.

---

### 26(c) Functions of Queues in Push-Pull Communication Model
### [(CO6)(Apply/IOCQ)]

In the **Push-Pull Communication Model**, **message queues** manage data flow between producers and consumers. Their primary functions include:

1. **Decoupling Producers and Consumers** - Queues act as an intermediary, allowing producers to send messages without waiting for consumers.
2. **Load Balancing** - Messages are stored in a queue until a consumer is ready, preventing system overload.
3. **Message Persistence** - Queues store messages temporarily, ensuring reliable delivery even if a consumer is unavailable.
4. **Prioritization** - Some queue implementations allow prioritizing urgent messages for faster processing.
5. **Scalability** - Multiple consumers can pull messages from the queue, improving system performance under high loads.
6. **Fault Tolerance** - If a consumer fails, messages remain in the queue until successfully processed by another consumer.

This model is widely used in **distributed systems, cloud services, and IoT networks** to improve communication reliability and efficiency.

---

### 27(a) Comparison of REST-Based Communication APIs vs. WebSocket-Based Communication APIs
### [(CO6)(Analyse/HOCQ)]

| Feature                | REST-Based Communication API | WebSocket-Based Communication API |
|------------------------|-----------------------------|-----------------------------------|
| **Communication Type** | Request-Response (Synchronous) | Full-Duplex (Asynchronous) |
| **Connection**         | Stateless, new connection for each request | Persistent, single connection |
| **Data Transfer**      | Each request gets a separate response | Continuous data exchange |
| **Latency**           | Higher due to repeated handshakes | Lower since connection is persistent |
| **Best Use Case**      | Suitable for standard web applications (e.g., RESTful APIs) | Ideal for real-time applications (e.g., chat, gaming, stock updates) |
| **Scalability**       | Scales well with caching and load balancing | Requires persistent connections, may increase resource consumption |

### Key Analysis:
- **REST is best for request-response interactions**, while **WebSockets are optimal for real-time bidirectional communication**.
- **WebSockets reduce latency and bandwidth** but require **more resources** to maintain persistent connections.
- REST is widely used in **web services and IoT APIs**, while WebSockets are better for **real-time IoT applications** like smart home automation and live monitoring.

---

### 27(b) Exclusive Pair Communication Model
### [(CO6)(Remember/LOCQ)]

The **Exclusive Pair Communication Model** is a **one-to-one communication mechanism** where two devices establish a **direct and persistent** connection. 

### Features:
1. **Dedicated Connection** - A stable, continuous link between two devices.
2. **Low Latency** - Ensures fast and reliable data transfer.
3. **Secure Communication** - Encryption and authentication mechanisms enhance security.
4. **Limited Scalability** - Not suitable for systems requiring multiple concurrent connections.
5. **Example Use Cases** - Used in **Bluetooth Pairing, Secure IoT Messaging, and Encrypted Device Communication**.

This model is **best suited for IoT applications where data privacy, reliability, and low latency are critical**.

---

### 27(c) Applications of IoT Enabling Technologies
### [(CO5)(Apply/IOCQ)]

Several technologies enable IoT applications across industries. Some key applications include:

1. **Smart Homes & Cities** - IoT-enabled lighting, energy management, and smart grids.
2. **Healthcare & Wearables** - Remote patient monitoring, smartwatches, and fitness trackers.
3. **Industrial Automation** - Predictive maintenance, robotics, and supply chain tracking.
4. **Agriculture** - Smart irrigation systems, soil monitoring, and automated farming.
5. **Transportation & Logistics** - Connected vehicles, GPS tracking, and fleet management.
6. **Retail & Customer Experience** - Smart shelves, personalized shopping experiences, and automated checkout systems.

These applications leverage IoT technologies like **RFID, Cloud Computing, Edge AI, 5G, and Blockchain** to improve efficiency, automation, and connectivity.

---

### 28(a) Comparison of Infrastructure as a Service (IaaS) vs. Platform as a Service (PaaS)
### [(CO6)(Analyse/HOCQ)]

| Feature                | Infrastructure as a Service (IaaS) | Platform as a Service (PaaS) |
|------------------------|----------------------------------|------------------------------|
| **Definition**        | Provides virtualized computing resources over the cloud. | Provides a development environment with tools and services. |
| **Control Level**     | Full control over the infrastructure (servers, storage, networking). | Limited control, focuses on application development and deployment. |
| **Management**       | Users manage operating systems, applications, and storage. | Cloud provider manages OS, runtime, and middleware. |
| **Scalability**       | Highly scalable, users can configure resources as needed. | Scales automatically but within the provider’s framework. |
| **Target Users**      | IT administrators, system architects needing custom infrastructure. | Developers focusing on building and deploying applications. |
| **Examples**         | AWS EC2, Google Compute Engine, Microsoft Azure Virtual Machines. | Google App Engine, AWS Elastic Beanstalk, Microsoft Azure App Services. |

### Key Analysis:
- **IaaS is best for users who need full control over their infrastructure**, while **PaaS is ideal for developers who want a ready-to-use development platform**.
- **IaaS requires more configuration**, whereas **PaaS simplifies development with pre-configured tools**.

---

### 28(b) Cloud Computing Services Offered to Users
### [(CO6)(Remember/LOCQ)]

Cloud computing offers various services categorized into the following:

1. **Infrastructure as a Service (IaaS)** - Provides virtual machines, storage, and networking (e.g., AWS EC2, Google Compute Engine).
2. **Platform as a Service (PaaS)** - Offers a development platform with pre-built tools (e.g., AWS Elastic Beanstalk, Heroku).
3. **Software as a Service (SaaS)** - Provides ready-to-use software over the cloud (e.g., Google Drive, Microsoft 365, Dropbox).
4. **Function as a Service (FaaS)** - Supports serverless computing where functions run on-demand (e.g., AWS Lambda, Google Cloud Functions).
5. **Storage as a Service (STaaS)** - Offers cloud-based storage solutions (e.g., Google Drive, OneDrive, Amazon S3).
6. **Database as a Service (DBaaS)** - Provides managed database services (e.g., AWS RDS, Firebase, Google Cloud SQL).

These services help users **reduce infrastructure costs, increase scalability, and enhance accessibility** in cloud environments.

---

### 28(c) Characteristics of Big Data
### [(CO5)(Analyse/IOCQ)]

Big Data is characterized by the **5Vs**:

1. **Volume** - Large amounts of data generated from various sources like sensors, healthcare devices, and IoT systems.
2. **Velocity** - High-speed data generation and processing in real-time or near real-time.
3. **Variety** - Data comes in multiple formats: structured (databases), semi-structured (JSON, XML), and unstructured (videos, images, logs).
4. **Veracity** - Ensures data accuracy and reliability, as sensor data may contain noise or inconsistencies.
5. **Value** - Extracting meaningful insights and patterns to drive decision-making and innovations.

Big Data technologies process and analyze **massive datasets** to improve efficiency in **healthcare, IoT, predictive maintenance, and AI applications**.

---

### 29(a) Four Stages of IoT Architecture System
### [(CO4)(Analyse/HOCQ)]

IoT systems typically follow a **four-stage architecture**, ensuring seamless data collection, processing, and decision-making.

### **1. Perception Layer (Sensing Layer)**
- Comprises **sensors, actuators, and edge devices** that collect real-world data.
- Examples: Temperature sensors, RFID tags, cameras, GPS modules.
- **Function**: Detects physical parameters and transmits data to the next stage.

### **2. Network Layer**
- Transmits data from the perception layer to the processing system.
- Uses **Wi-Fi, Bluetooth, Zigbee, LoRaWAN, Cellular (4G/5G), or Ethernet**.
- **Function**: Ensures secure and reliable communication between IoT devices and cloud servers.

### **3. Processing Layer (Edge or Fog Computing Layer)**
- Handles **local data processing and decision-making** before sending data to the cloud.
- Includes **edge servers, IoT gateways, and microcontrollers (Arduino, Raspberry Pi, ESP32).**
- **Function**: Reduces latency and network congestion by pre-processing data.

### **4. Application Layer**
- Provides a user interface for monitoring and controlling IoT systems.
- Examples: **Mobile apps, web dashboards, voice assistants (Alexa, Google Assistant).**
- **Function**: Translates data into meaningful insights and enables user interaction.

Each stage works together to create an efficient **end-to-end IoT ecosystem**, ensuring real-time monitoring, automation, and decision-making.

---

### 29(b) Suitable System for Low-Cost, Low-Complexity Solutions
### [(CO5)(Apply/IOCQ)]

For **low-cost and low-complexity IoT applications** like **home automation and smart irrigation**, a suitable system should have:

### **1. Microcontroller-Based Architecture**
- **ESP32 / ESP8266 / Arduino Uno** for simple automation tasks.
- **LoRa / Zigbee / Wi-Fi** for low-power wireless communication.

### **2. Lightweight Data Processing**
- **Edge computing** using **Raspberry Pi** or ESP32 to process small datasets.
- **No need for heavy cloud computing**; simple **local storage (SD card) or Firebase** is sufficient.

### **3. Energy-Efficient and Cost-Effective Sensors**
- **Soil moisture sensors** for smart irrigation.
- **Motion and light sensors** for home automation.
- **DHT11/DHT22 temperature and humidity sensors** for climate control.

### **4. Low-Power Communication Protocols**
- **MQTT (Message Queuing Telemetry Transport)** for lightweight IoT messaging.
- **Bluetooth Low Energy (BLE) or LoRaWAN** for minimal power consumption.

### **Example Use Case**: **Smart Irrigation System**
- **ESP32 collects soil moisture data** and controls water flow.
- **LoRa transmits data to a remote monitoring system**.
- **Mobile app displays real-time soil conditions and automation status**.

By utilizing **low-power microcontrollers, edge computing, and efficient sensors**, such solutions offer an optimal balance between **cost, power consumption, and performance**.

---

### 30(a) IoT System for Noise Monitoring
### [(CO5)(Analyse/HOCQ)]

An IoT-based noise monitoring system is designed to detect and analyze noise levels in different environments, such as **urban areas, industries, and residential zones**. 

### **System Components:**
1. **Sound Sensors (Microphones & Decibel Meters)** - Captures real-time noise levels.
2. **Microcontrollers (ESP32/Raspberry Pi/Arduino)** - Processes the sensor data.
3. **Wireless Communication (Wi-Fi, LoRa, or GSM)** - Sends data to cloud storage.
4. **Cloud/Edge Computing (AWS IoT, Google Cloud IoT, ThingSpeak)** - Analyzes noise trends.
5. **Dashboard & Alerts (Mobile App/Web Interface)** - Displays noise levels and triggers notifications when thresholds are exceeded.

### **Working Mechanism:**
- The **sound sensors measure noise intensity** and send data to a microcontroller.
- The processed data is **transmitted to the cloud** for analysis and visualization.
- If noise levels exceed a predefined limit, **alerts are sent to authorities or users**.
- The system can be used for **pollution monitoring, workplace safety, and smart city initiatives**.

---

### 30(b) IoT-Specific Home Automation Systems
### [(CO5,CO6)(Remember/LOCQ)]

Several IoT-based home automation solutions are widely used for smart living. Some of the key systems include:

1. **Smart Lighting (Philips Hue, LIFX)** - IoT-enabled bulbs controlled via mobile apps or voice assistants.
2. **Smart Thermostats (Nest, Ecobee)** - Automatically adjust home temperature for energy efficiency.
3. **Smart Security Cameras (Ring, Arlo, Wyze)** - Motion-detecting cameras for home surveillance.
4. **Smart Door Locks (August, Yale, Schlage Encode)** - IoT-based keyless entry with remote control.
5. **Smart Plugs (TP-Link Kasa, Wemo)** - Allows remote control of appliances via IoT connectivity.
6. **Voice Assistants (Amazon Alexa, Google Assistant, Apple Siri)** - Centralized control for smart home devices.

---

### 30(c) IoT-Based City Automation Applications
### [(CO5,CO6)(Apply/IOCQ)]

1. **Smart Traffic Management** - IoT sensors and AI optimize traffic flow, reducing congestion and improving transportation efficiency.
2. **Smart Waste Management** - IoT-enabled trash bins send fill-level data to optimize waste collection routes, reducing operational costs.

These applications enhance urban infrastructure, leading to **efficient, sustainable, and connected smart cities**.

### 31(a) Comparison: Air Pollution Monitoring vs. Noise Pollution Monitoring in IoT
### [(CO5)(Analyse/HOCQ)]

| Feature                | Air Pollution Monitoring System | Noise Pollution Monitoring System |
|------------------------|--------------------------------|----------------------------------|
| **Purpose**           | Monitors air quality by detecting pollutants like CO₂, NO₂, PM2.5, etc. | Measures noise levels in dB to assess environmental noise pollution. |
| **Key Sensors**       | Gas sensors (MQ-135, CO₂ sensors), PM2.5 sensors | Microphones, decibel meters (MEMS, Electret Condenser Microphones). |
| **Data Processing**   | Analyzes pollutant concentration trends over time. | Detects noise intensity variations and threshold breaches. |
| **Real-time Alerts**  | Sends warnings if pollution exceeds safe levels. | Notifies authorities if noise pollution surpasses limits. |
| **Use Cases**        | Smart cities, industrial emissions control, vehicle pollution tracking. | Urban noise pollution management, workplace safety, smart campuses. |
| **Impact**           | Reduces respiratory diseases and improves public health. | Helps enforce noise regulations and prevents hearing-related issues. |

Both systems contribute to **environmental sustainability** and **public well-being** under the IoT domain.

---

### 31(b) Machine-to-Machine (M2M) Communication
### [(CO6)(Remember/LOCQ)]

M2M (Machine-to-Machine) communication refers to the **direct exchange of data between devices** without human intervention. It enables automation across various industries using IoT networks.

### **Key Characteristics:**
- **Autonomous Data Exchange** - Devices communicate in real time without manual input.
- **Network Connectivity** - Uses Wi-Fi, Zigbee, LoRa, Cellular (4G/5G), or LPWAN.
- **Remote Monitoring & Control** - Sensors and actuators work together to optimize operations.
- **Minimal Human Intervention** - Reduces errors and enhances efficiency.

M2M plays a crucial role in **smart industries, healthcare, agriculture, and transportation** by enabling seamless automation.

---

### 31(c) Two M2M Applications
### [(CO6)(Apply/IOCQ)]

1. **Smart Metering** - Automated electricity, water, and gas meters transmit consumption data to utility providers, improving billing accuracy and resource management.
2. **Fleet Management** - IoT-enabled vehicle tracking systems monitor fuel usage, route optimization, and maintenance schedules for logistics companies.

M2M applications **enhance operational efficiency, reduce costs, and improve decision-making** in various industries.

### 32(a) Difference Between IoT and M2M Features
### [(CO4)(Analyse/HOCQ)]

| Feature               | Internet of Things (IoT)         | Machine-to-Machine (M2M)       |
|-----------------------|--------------------------------|--------------------------------|
| **Definition**       | A network of connected devices that communicate over the internet. | Direct communication between machines without human intervention. |
| **Communication**    | Uses **IP-based networks (Wi-Fi, LPWAN, 5G, etc.)**. | Uses **dedicated point-to-point connections (Cellular, Zigbee, RFID, etc.)**. |
| **Data Handling**    | Processes data on **cloud-based platforms** for analytics and decision-making. | Data is **exchanged locally** between devices with minimal cloud involvement. |
| **Automation**       | Provides **AI-driven automation and predictive analytics**. | Automates repetitive tasks but **lacks AI-based intelligence**. |
| **Scalability**      | Highly **scalable** and supports billions of devices. | Limited scalability, mainly used in industrial and enterprise settings. |
| **Example Applications** | Smart homes, healthcare monitoring, autonomous vehicles. | Industrial automation, fleet tracking, remote asset monitoring. |

IoT extends M2M functionality by integrating cloud computing, data analytics, and AI for enhanced automation.

---

### 32(b) General Architecture of an M2M System
### [(CO4)(Remember/LOCQ)]

M2M systems consist of multiple components that facilitate **machine-to-machine communication**. The architecture generally includes:

### **1. Device Layer (Machines & Sensors)**
- Comprises physical devices like **sensors, actuators, meters, and controllers**.
- Collects data from the environment (e.g., temperature, pressure, GPS location).

### **2. Communication Layer (Network & Protocols)**
- Transfers data between devices using **cellular networks (4G/5G), LPWAN (LoRa), Zigbee, or RFID**.
- Securely transmits information to processing systems.

### **3. Data Management Layer (Middleware & Processing)**
- Edge gateways or servers filter and process raw data before storage.
- Data transmission protocols like **MQTT, CoAP, or HTTP** are used for connectivity.

### **4. Application Layer (User Interfaces & Decision Systems)**
- Data is analyzed and presented on **dashboards, mobile apps, or cloud platforms**.
- Decision-making and control automation take place here.

This architecture allows M2M systems to operate **efficiently, securely, and in real-time** for various industrial applications.

---

### 32(c) Machines in M2M vs. Things in IoT
### [(CO5)(Apply/IOCQ)]

- **Machines in M2M** are typically **pre-programmed devices** like industrial robots, smart meters, and GPS trackers that operate **autonomously in a fixed network**.
- **Things in IoT** include **smart objects** like wearables, smart home devices, and connected cars that interact with **cloud-based AI systems for real-time decision-making**.

M2M focuses on **direct machine interaction**, while IoT enhances **connectivity, intelligence, and analytics** for broader applications.

### 33(a) Comparison: Data Plane vs. Control Plane in SDN
### [(CO4)(Analyse/HOCQ)]

| Feature            | Data Plane                         | Control Plane                     |
|--------------------|----------------------------------|----------------------------------|
| **Definition**     | Handles packet forwarding and processing. | Manages network rules and traffic policies. |
| **Function**       | Directly processes incoming and outgoing packets. | Determines how data should be forwarded. |
| **Location**       | Found in network devices like routers and switches. | Resides in a centralized SDN controller. |
| **Decision Making**| Works based on instructions from the control plane. | Uses algorithms and policies to optimize routing. |
| **Scalability**    | Scales with hardware-based forwarding capabilities. | Scales through cloud-based or distributed controllers. |
| **Example**       | OpenFlow switches executing forwarding rules. | SDN controllers (e.g., OpenDaylight, ONOS) managing routing. |

The **data plane** is responsible for the **actual movement of packets**, while the **control plane** defines how packets should be handled, improving network flexibility and programmability.

---

### 33(b) Importance of SDN
### [(CO4)(Remember/LOCQ)]

SDN (Software-Defined Networking) is important because it **decouples network control from hardware**, providing benefits such as:

1. **Centralized Control** – SDN enables a single controller to manage multiple network devices, reducing complexity.
2. **Improved Scalability** – Dynamic allocation of resources makes networks more adaptable to varying loads.
3. **Flexibility & Programmability** – Administrators can update and modify network policies without changing hardware.
4. **Enhanced Security** – Centralized monitoring and policy enforcement prevent unauthorized access and threats.
5. **Optimized Performance** – SDN allows intelligent traffic routing and load balancing for better efficiency.

SDN plays a crucial role in **cloud computing, data centers, and 5G networks**, making networking more agile and cost-effective.

---

### 33(c) Three Main Components of SDN
### [(CO4)(Apply/IOCQ)]

SDN consists of three key components:

1. **Application Layer**
   - Includes **network applications** that define services such as traffic routing, firewall rules, and security policies.
   - Examples: Load balancers, intrusion detection systems, and monitoring tools.

2. **Control Layer (SDN Controller)**
   - Acts as the **brain of the SDN** by managing network policies and forwarding rules.
   - Communicates with both the application layer (above) and the infrastructure layer (below).
   - Example Controllers: **OpenDaylight, ONOS, Ryu**.

3. **Infrastructure Layer (Data Plane)**
   - Comprises **physical and virtual network devices** such as routers, switches, and access points.
   - These devices forward traffic based on rules set by the SDN controller.
   - Uses **OpenFlow, NETCONF, or other southbound APIs** for communication.

Together, these layers **enable programmable, agile, and efficient networking**, making SDN a transformative technology in modern networking.

### 34(a) Comparison: SDN vs. Traditional Networking
### [(CO4)(Analyse/HOCQ)]

| Feature                 | Traditional Networking                        | Software-Defined Networking (SDN) |
|-------------------------|----------------------------------------------|----------------------------------|
| **Network Control**     | Control plane is embedded in network devices. | Control plane is centralized in an SDN controller. |
| **Flexibility**         | Static configuration with limited adaptability. | Highly programmable and flexible. |
| **Scalability**         | Difficult to scale due to hardware dependence. | Easily scalable with cloud-based management. |
| **Management**         | Managed individually on each device. | Centralized control for easier management. |
| **Traffic Handling**     | Uses traditional routing protocols (OSPF, BGP). | Dynamic flow-based traffic control. |
| **Security**            | Security policies applied per device. | Global security enforcement through the controller. |
| **Implementation Cost** | High due to specialized hardware. | Lower cost using commodity hardware. |

SDN provides a **more agile, cost-effective, and scalable** approach to networking compared to traditional networking.

---

### 34(b) Advantages of SDN
### [(CO4)(Remember/LOCQ)]

SDN offers multiple benefits that enhance network efficiency and management:

1. **Centralized Control** – Administrators can manage the entire network from a single point.
2. **Flexibility** – Dynamic policy updates without modifying physical devices.
3. **Cost-Effective** – Uses general-purpose hardware instead of expensive network devices.
4. **Improved Security** – Centralized monitoring and security enforcement.
5. **Optimized Traffic Management** – Intelligent routing and load balancing improve performance.
6. **Automation** – Reduces manual configuration through network programming.

SDN simplifies **network operations and enhances adaptability** for modern IT environments.

---

### 34(c) SDN Architecture
### [(CO4)(Apply/IOCQ)]

SDN architecture consists of three key layers:

### **1. Application Layer**
- Contains **network applications** that define services like security, routing, and monitoring.
- Examples: Firewalls, Intrusion Detection Systems (IDS), and QoS applications.

### **2. Control Layer (SDN Controller)**
- Acts as the **central intelligence** of the SDN.
- Manages and configures network behavior using southbound APIs (e.g., OpenFlow).
- Example Controllers: **ONOS, OpenDaylight, Ryu**.

### **3. Infrastructure Layer (Data Plane)**
- Comprises physical and virtual networking devices such as **switches, routers, and access points**.
- Forwards data packets based on the SDN controller's instructions.

### **Communication Interfaces**
- **Northbound APIs**: Connect applications to the SDN controller (e.g., REST APIs).
- **Southbound APIs**: Connect controllers to network devices (e.g., OpenFlow, NETCONF).

This layered architecture **enhances network programmability, automation, and efficiency**, making SDN an essential technology for modern networks.

### 35(a) Comparison: SDN vs. Traditional Networking
### [(CO4)(Analyse/HOCQ)]

| Feature                 | Traditional Networking                        | Software-Defined Networking (SDN) |
|-------------------------|----------------------------------------------|----------------------------------|
| **Network Control**     | Embedded in each network device. | Centralized in an SDN controller. |
| **Flexibility**         | Rigid and static configurations. | Highly programmable and flexible. |
| **Scalability**         | Limited due to hardware dependency. | Easily scalable with virtualized controllers. |
| **Management**         | Managed per device. | Centrally managed via controllers. |
| **Traffic Handling**     | Uses traditional routing protocols. | Implements dynamic flow-based forwarding. |
| **Security**            | Implemented device by device. | Centralized security policies. |
| **Cost** | High, requires specialized hardware. | Lower, can use general-purpose hardware. |

SDN **separates the control plane from the data plane**, making networks more programmable and efficient.

---

### 35(b) Advantages of Network Function Virtualization (NFV)
### [(CO5)(Remember/LOCQ)]

Network Function Virtualization (NFV) offers several advantages:

1. **Cost Savings** – Replaces dedicated network hardware with software-based virtual network functions (VNFs), reducing capital and operational expenses.
2. **Flexibility & Scalability** – Services can be scaled up or down dynamically based on demand.
3. **Rapid Deployment** – New network functions can be deployed quickly without hardware changes.
4. **Automation & Orchestration** – Simplifies network management by automating service deployment and scaling.
5. **Improved Network Efficiency** – Optimizes resource allocation by running multiple virtualized functions on the same physical infrastructure.
6. **Reduced Power Consumption** – Uses fewer physical devices, leading to energy savings.

NFV **enhances the agility of network services**, making it ideal for cloud and telecom applications.

---

### 35(c) Working of NFV (with Block Diagram)
### [(CO5)(Apply/IOCQ)]

### **NFV Architecture & Workflow**
NFV is built on three primary components:

1. **Virtualized Network Functions (VNFs)**
   - Software-based implementations of network functions (e.g., firewalls, routers, load balancers).
2. **NFV Infrastructure (NFVI)**
   - Provides the compute, storage, and networking resources needed to run VNFs.
3. **NFV Management and Orchestration (NFV-MANO)**
   - Handles lifecycle management, automation, and orchestration of VNFs.

### **Block Diagram of NFV Architecture**
![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/nfvarch.png?raw=true)

### **How NFV Works:**
1. **A user requests a network function (e.g., firewall, VPN, load balancer).**
2. **NFV Orchestration deploys the requested VNFs on available NFVI resources.**
3. **The VNF runs in a virtualized environment, processing network traffic dynamically.**
4. **Scaling and optimization are handled automatically by NFV-MANO.**

NFV improves network **efficiency, automation, and cost-effectiveness**, making it a key technology in modern telecom and cloud computing.

### 36(a) Properties of Sensors: Range, Sensitivity, and Resolution
### [(CO5)(Analyse/HOCQ)]

Sensors are key components of IoT systems. The most important properties are:

1. **Range**:
   - Defines the minimum and maximum values the sensor can measure.
   - Example: A temperature sensor with a range of **-40°C to 125°C**.

2. **Sensitivity**:
   - The smallest change in input that causes a detectable change in output.
   - Example: A pressure sensor with **1 Pa sensitivity** detects very small pressure variations.

3. **Resolution**:
   - The smallest measurable unit of the sensor output.
   - Example: A digital thermometer with **0.1°C resolution** can distinguish changes as small as 0.1°C.

Higher **sensitivity and resolution** improve precision, while an appropriate **range** ensures applicability for various conditions.

---

### 36(b) Advantages of Network Function Virtualization (NFV)
### [(CO5)(Remember/LOCQ)]

NFV provides several key benefits:

1. **Reduced Hardware Dependency** – Eliminates reliance on physical network devices by virtualizing functions.
2. **Cost Efficiency** – Decreases capital and operational expenses by using software-based network functions.
3. **Scalability & Flexibility** – Quickly deploy and adjust network services on demand.
4. **Improved Automation** – Simplifies network management with automated provisioning and orchestration.
5. **Faster Service Deployment** – Enables rapid rollout of new network services without hardware changes.
6. **Optimized Resource Utilization** – Allocates compute and networking resources dynamically, improving efficiency.

NFV **modernizes network infrastructure** and enhances service agility for cloud-based and telecom environments.

---

### 36(c) Working of NFV (with Block Diagram)
### [(CO5)(Apply/IOCQ)]

### **NFV Architecture & Workflow**
NFV consists of three main components:

1. **Virtualized Network Functions (VNFs)** – Software-based network services (e.g., firewalls, routers, load balancers).
2. **NFV Infrastructure (NFVI)** – Compute, storage, and networking resources where VNFs are deployed.
3. **NFV Management & Orchestration (NFV-MANO)** – Manages lifecycle, automation, and deployment of VNFs.

### **Block Diagram of NFV Architecture**
![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/nfvarch.png?raw=true)

### **How NFV Works:**
1. **A network function (e.g., firewall) is requested.**
2. **The NFV Orchestrator assigns resources and deploys VNFs.**
3. **The virtual function processes network traffic dynamically.**
4. **Scaling and optimization are managed automatically.**

NFV enhances **network agility, cost-efficiency, and automation**, making it essential for modern telecom and cloud computing networks.

### 37(a) Advantages and Disadvantages of NFV
### [(CO5)(Analyse/HOCQ)]

### **Advantages of Network Function Virtualization (NFV):**
1. **Cost Reduction** – Eliminates the need for dedicated network hardware, reducing CAPEX and OPEX.
2. **Scalability** – Virtualized functions can be dynamically scaled up or down as needed.
3. **Flexibility** – Enables rapid deployment of new network services without hardware changes.
4. **Automation & Orchestration** – Simplifies network management through automated provisioning.
5. **Energy Efficiency** – Reduces power consumption by running multiple VNFs on shared infrastructure.
6. **Improved Service Agility** – Faster rollout of new services and network functions.

### **Disadvantages of NFV:**
1. **Performance Overhead** – Virtualized functions may have slightly lower performance compared to hardware-based solutions.
2. **Security Concerns** – NFV introduces additional attack vectors due to software vulnerabilities.
3. **Complex Management** – Requires efficient orchestration and monitoring to ensure smooth operation.
4. **Integration Challenges** – Legacy network components may not be fully compatible with NFV-based systems.
5. **Reliability Risks** – A failure in the NFV infrastructure can impact multiple virtualized network functions at once.

---

### 37(b) Steps of IoT Methodology
### [(CO3)(Remember/LOCQ)]

IoT systems follow a structured methodology for data collection, processing, and action. The key steps are:

1. **Perception Layer (Sensing & Data Collection):**
   - Sensors and devices collect real-world data (e.g., temperature, motion, humidity).
2. **Network Layer (Data Transmission):**
   - Transfers collected data to cloud or edge servers via WiFi, Bluetooth, LPWAN, or cellular networks.
3. **Edge Computing (Pre-Processing & Filtering):**
   - Processes raw data at the network edge to reduce latency before sending it to the cloud.
4. **Cloud Processing & Storage:**
   - The cloud stores and analyzes the data using AI, ML, or big data analytics.
5. **Application Layer (User Interaction & Action):**
   - The processed data is used to trigger actions (e.g., sending alerts, controlling actuators, displaying insights).
6. **Security & Management:**
   - Ensures data encryption, authentication, and real-time monitoring to prevent cyber threats.

IoT methodology **enables seamless connectivity, real-time processing, and intelligent decision-making.**

---

### 37(c) NFV Architecture
### [(CO4)(Apply/IOCQ)]

### **NFV Architecture Overview**
The NFV architecture consists of three major components:

1. **Virtualized Network Functions (VNFs):**
   - Software-based implementations of network functions (e.g., routers, firewalls, load balancers).
2. **NFV Infrastructure (NFVI):**
   - Provides the compute, storage, and network resources needed to run VNFs.
3. **NFV Management and Orchestration (NFV-MANO):**
   - Responsible for the lifecycle management, automation, and orchestration of VNFs.

### **NFV Architecture Block Diagram**
![alt text](https://github.com/aditya95-pixel/Big-Data-Sem-prep/blob/main/nfvarch.png?raw=true)

### **How NFV Works:**
1. **Network operators request a virtual function (e.g., firewall, VPN).**
2. **NFV-MANO assigns resources and deploys the required VNFs.**
3. **VNFs process network traffic dynamically based on user demands.**
4. **The system auto-scales, optimizes, and manages resources efficiently.**

NFV **transforms traditional networking by enabling virtualization, automation, and cost reduction.** 
